% ==============================================================================
% This file is part of the "LaTeX template for writing the Final Degree Work
% report". It has been developed to aid the students of the Bachelor's Degree in
% Video Game Design and Development at the Jaume I University.
%
% (c) 2019 Sergio Barrachina Mir and José Vte. Martí Avilés
%
% The template can be used and distributed under the next license:
%  Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)
%  http://creativecommons.org/licenses/by-nc-sa/3.0/
%  http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode
%
% Atom editor configuration follows:
% !TEX root = ./report.tex
% !TeX spellcheck = en-US
% ==============================================================================
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\chapter{Reactive movements}
\label{app:react}

This appendix contains some notes that were taken when (and after) training sessions related to modeling reactive behaviors.

Any parameter that does not appear in one training is set to default (see ./config/trainer.config file).

Trained models with green titles are considered good or any improvement in the investigation. Models with red titles are considered failures.

Some of the models have not been saved, either because they don't perform well or they perform in much the same way as another model (agent).

Most of the notes should not be taken literally or as certain, as they usually are theories or preliminary conclusions drawn during the training itself.
 
\section*{\color{red} 20/4 10:42 (4\textunderscore 20\textunderscore 1042)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e5\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 100 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Camera Sensor: 40x40

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 3

\vspace{2mm}

Episode steps = 3000

Total steps = 51400

Time = 1000s

\vspace{2mm}

It may be necessary to decrease the action vector to only 1 (whichever it is; the clicks are treated as automatic). Improve coherence of movement.

\section*{\color{red} 20/4 13:20 (4\textunderscore 20\textunderscore 1320)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e5\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			\textbf{summary freq} & \textbf{1000} & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Camera Sensor: 40x40

Grayscale: false

Observation space size = 25 (eje X)

\textbf{Action space size = 1}

\vspace{2mm}

Episode steps = 3000

Total steps = 60000

Time = 1200s

\vspace{2mm}

It needs more training, or different parameters. It looks like it's moving in the direction but it's still ``vibrating'' a lot. It may be necessary to think about raytracing instead of a camera.

\section*{\color{red} 20/4 13:57 (4\textunderscore 20\textunderscore 1357)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & \textbf{max steps} & \textbf{5.0e6}\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Camera Sensor: 40x40

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 170000

Time = 2200s (+)

\vspace{2mm}

Same agent as 4\textunderscore 20\textunderscore 1320 (checkpoint)

\section*{\color{red} 20/4 14:40 (4\textunderscore 20\textunderscore 1440)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Camera Sensor: 40x40

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 500000

Time = 6770s (+)

\vspace{2mm}

It seems to have stabilized at a reward of 12,500 - 12,600. 

In reality it stays practically paralyzed, moving very little (almost nothing).

\section*{\color{red} 21/4 12:32 (4\textunderscore 21\textunderscore 1232)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

\textbf{Render Target Sensor: 64x64}

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 350000

Time = 7530s

\vspace{2mm}

The Bot has been improved with reaction time and more speed. More demanding reward conditions.

The graph seems to show good behavior. Keeps training.

\section*{\color{red} 21/4 14:44 (4\textunderscore 21\textunderscore 1444)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = -

Time = -

\vspace{2mm}

Same agent as 4\textunderscore 21\textunderscore 1232.

It doesn't work properly, but if you start the bot the graph works as it should. On the other hand, although the graph follows the same patterns, it stagnates at an amount below (maybe the movement limit). It also causes untrained cases to occur.

\section*{\color{red} 21/4 17:31 (4\textunderscore 21\textunderscore 1731)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 310000

Time = 6700s

\vspace{2mm}

Fixed that the agent reads half of the movement value (bug), instead of what the bot actually returned.

I think it uses the previous movements as "cheat sheet" to know what to do, maybe you have to detect the previous movements of the agent to learn or make the agent to move, and the bot to correct.

-I had to interrupt the training because the reaction was coming out of the graph (now it's -2 to 2, instead of 1).

-Another interruption, the vector action only returns normalized, so you have to change the value that happens to the camera (not the other way around).

The graph fits perfectly in the training, but I think it's because you can see what the real bot has done in the previous movement. It may be necessary to add curiosity rewards in following trainings. 

The model seemed to make a good default move, even with a change of direction, but then it tends to turn at full speed without stopping. It should certainly stop reading what the bot does in the last move.

\section*{\color{red} 22/4 10:00 (4\textunderscore 22\textunderscore 1000)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.95}\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25 (eje X)

Action space size = 1

\vspace{2mm}

\textbf{Episode steps = 2000}

Total steps = 41000

Time = 900s

\vspace{2mm}

The previous movements have been changed to those of the agent.

It tends towards chaos.

\section*{\color{red} 22/4 10:31 (4\textunderscore 22\textunderscore 1031)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 5.0e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.9}\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

\textbf{Observation space size = 0}

Action space size = 1

\vspace{2mm}

\textbf{Episode steps = 3000}

Total steps = 185000

Time = 4000s

\vspace{2mm}

All observation other than the camera has been removed. The bot reacts in time 0 now. An idle address has also been removed.

It was on the right track but tends to go into chaos after 1h.

\section*{\color{green} 22/4 11:43 (4\textunderscore 22\textunderscore 1143)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			\textbf{learning rate} & \textbf{2.5e-4} & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			\textbf{curiosity strength} & \textbf{0.01} & \textbf{curiosity gamma} & \textbf{0.99}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{256} & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 180000

Time = 4100s

\vspace{2mm}

Decreased idle randomness and increased acceptable range.
 
The graph fits well, although with some noise (perhaps out of curiosity), but it also tends to hit the planes that appear on the opposite side (this didn't happen before, it only reacted well to those that appeared in front). In viewport it also seems very similar, and the noise is not so noticeable.

Saved as a checkpoint.

--Note: the behaviour is a bit different because when training the screen is displayed in a 16:9 ratio (instead of 1:1 from the editor), that makes the camera react a bit differently.

\section*{\color{red} 22/4 13:19 (4\textunderscore 22\textunderscore 1319)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			learning rate & 2.5e-4 & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 256000

Time = 1500s (+)

\vspace{2mm}

Correct the proportion in the build. Actually, as the render target's camera goes out from the agent's one, it doesn't cause problems. However the isVisible should be changed in relation to that camera.

It tends again to chaos.

\section*{\color{green} 22/4 13:52 (4\textunderscore 22\textunderscore 1352)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256\\
		\hline
			\textbf{learning rate} & \textbf{1.0e-4} & max steps & 5.0e6\\
		\hline
			normalize & false & num layers & 1\\
		\hline
			summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 373000

Time = 8050s

\vspace{2mm}

Before training the reward is increased by being in range.

VERY slow training, consider setting a ``learning rate schedule'' so that the learning rate goes from more to less (but it may already be by default, you would have to decrease the number of training steps).

It seems more or less stable (like the previous one achieved).

\section*{\color{red} 22/4 16:15 (4\textunderscore 22\textunderscore 1615)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & \textbf{hidden units} & \textbf{128}\\
		\hline
			\textbf{learning rate} & \textbf{1.0e-3} & \textbf{learning rate schedule} & \textbf{linear}\\
		\hline
			\textbf{max steps} & \textbf{5.0e4} & normalize & false\\
		\hline
			\textbf{num layers} & \textbf{2} & summary freq & 1000\\
		\hline
			time horizon & 5 & extrinsic strength & 1.0\\
		\hline
			extrinsic gamma & 0.9 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 50000

Time = 1075s

\vspace{2mm}

Learning rate added (so that it can be changed explicitly) The steps are now determined. The structure of the network has also been changed.

It has reached a reward of -0.313, maybe for increasing the number of layers.

\section*{\color{red} 22/4 16:40 (4\textunderscore 22\textunderscore 1640)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & \textbf{hidden units} & \textbf{256}\\
		\hline
			learning rate & 1.0e-3 & learning rate schedule & linear\\
		\hline
			\textbf{max steps} & \textbf{1.0e5} & normalize & false\\
		\hline
			\textbf{num layers} & \textbf{1} & summary freq & 1000\\
		\hline
			time horizon & 5 & extrinsic strength & 1.0\\
		\hline
			extrinsic gamma & 0.9 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 100000

Time = 2160s

\vspace{2mm}

It may be necessary to lower the initial learning rate to increase the effectiveness of the training. In the first 50,000 steps it makes very little progress (from -.524 to -.415). 

In the end it reaches -.353.

\section*{\color{red} 22/4 17:19 (4\textunderscore 22\textunderscore 1719)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{1024} & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{4096} & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{2.0e-4} & \textbf{learning rate schedule} & \textbf{constant}\\
		\hline
			\textbf{max steps} & \textbf{2.0e5} & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{5} & num layers & 1\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{8}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 25000

Time = 540s

\vspace{2mm}

Several changes to adapt the network to continuous space. Constant Learning Rate to check when it converges (or diverges)
Does not converge after 25000 steps.

\section*{\color{red} 22/4 17:54 (4\textunderscore 22\textunderscore 1754)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{1.0e-4} & learning rate schedule & constant\\
		\hline
			\textbf{max steps} & \textbf{2.0e6} & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{3} & num layers & 1\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{32}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 115000

Time = 2420s

\vspace{2mm}

Long training to try to get it stabilized. 

--It doesn't stabilize after 100,000 steps, I think it has a bigger problem with how the rewards are given. I'll turn them into linear.

\section*{\color{red} 22/4 18:42 (4\textunderscore 22\textunderscore 1842)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{5.0e-4} & \textbf{learning rate schedule} & \textbf{linear}\\
		\hline
			max steps & 2.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 32\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 61000

Time = 1200s

\vspace{2mm}

It shows no sign of stabilizing, so the range of success may still be very demanding.

To save time in converging to the original bot function, I have thought to use learning curriculum that determines the thresholds of the rewards, making them smaller and smaller. 

\section*{\color{red} 23/4 10:49 (4\textunderscore 23\textunderscore 1049)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & hidden units & 256 \\
		\hline
			learning rate & 5.0e-4 & learning rate schedule & linear\\
		\hline
			max steps & 2.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 32\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 116000

Time = 2400s

\vspace{2mm}

Training with curriculum. Still having trouble converging.

\section*{\color{red} 23/4 12:46 (4\textunderscore 23\textunderscore 1246)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{32} & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{256} & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{3.0e-4} & learning rate schedule & linear\\
		\hline
			\textbf{max steps} & \textbf{1.0e6} & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{5}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

\textbf{Episode steps = 2000}

Total steps = 394000

Time = 8500s

\vspace{2mm}

Increased the minimum reward to pass the level. Changes in variables until a stable one is achieved.

It manages to stabilize the idle movement (after quite a while) but doesn't do impulses well.

\section*{\color{red} 23/4 15:51 (4\textunderscore 23\textunderscore 1551)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{1024} & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{4096} & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{5.0e-4} & learning rate schedule & linear\\
		\hline
			max steps & 1.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{8}\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.8}\\
		\hline
			\textbf{curiosity strength} & \textbf{0.05} & \textbf{curiosity gamma} & \textbf{0.9}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{128} & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 2000

Total steps = 453000

Time = 7300s

\vspace{2mm}

Added penalty for out of range (beyond maximumRange) and curiosity. Also the extrinsic gamma is lowered to make the present (at this time) count more.

Reaches level 2 with 0.4 reward but shows no improvement. The reward range may need to be increased.

\section*{\color{red} 23/4 19:15 (4\textunderscore 23\textunderscore 1915)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & \textbf{beta} & \textbf{5.0e-4} \\ 
		\hline
			buffer size & 4096 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{1.0e-3} & learning rate schedule & linear\\
		\hline
			max steps & 1.0e6 & normalize & false\\
		\hline
			num epoch & 3 & \textbf{num layers} & \textbf{2}\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{2048}\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.9}\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 2000

Total steps = 760000

Time = 16700s

\vspace{2mm}

Training by watching tensorboard. Test with 2 layers, it's probably slower.

The training is too slow, although the graphs seem to have correct progression. The cause may be the form of the reward function or the fact that it had a higher range before (up to -120).

\section*{\color{green} 24/4 10:26 (4\textunderscore 24\textunderscore 1026)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{32} & \textbf{beta} & \textbf{5.0e-3} \\ 
		\hline
			\textbf{buffer size} & \textbf{256} & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{2.5e-4} & learning rate schedule & linear\\
		\hline
			\textbf{max steps} & \textbf{5.0e6} & normalize & false\\
		\hline
			num epoch & 3 & \textbf{num layers} & \textbf{1}\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{5}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			\textbf{curiosity strength} & \textbf{0.01} & \textbf{curiosity gamma} & \textbf{0.99}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{256} & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

\textbf{Episode steps = 3000}

Total steps = 112000

Time = 2400s

\vspace{2mm}

Training with old reward functions (TR=0.005), and with the old parameters In 1500-2000s it starts to get reasonably close. The graphs don't show any strange behaviour, but the rewards are always negative (the network should break in 180k-200k steps, now it is at 100k). The model is correct.

\section*{\color{green} 24/4 16:00 (4\textunderscore 24\textunderscore 1600)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 435000

Time = 10000s

\vspace{2mm}

Training with the same parameters and without curriculum (with TR=0.01, MR=1). Using linear rewards but multiplied by 40. 

From 80000 steps or a little earlier you can see that it adapts to the form (except for some transitions between jumps). 

At 150000 steps it almost acts like one of the previous models: it goes from -16 to -2 reward and the graphs don't show anything strange.

At 190000 steps first positive reward (0.340).

At 275,000 steps the idle stages are virtually out of noise. However, it can be seen that the network does not fit well when there are more than 2 targets at once, this may be because of how the bot algorithm prioritizes them and the fact that the network still has no memory. The reward is 3.

Afterwards, the reward increases steadily because it adapts to the idle with more precision, but the more time goes by the less I am convinced about how it adapts to some impulses (it may be due to underfitting, since it doesn't have the memory installed yet either in image or in movements).

Putting it in Unity is less convincing because it doesn't follow the same movement if it's executed autonomously (it's smoothed out even more, the impulses are slower).

In the following tests: increase the learning-rate and put the values normalized; in another one increase even more the multiplication factor. Changing the shape of the function may improve (or worsen) the learning curve. After the 2 tests, it would be convenient to put the movement memory back (25 previous values).

\section*{\color{red} 24/4 19:35 (4\textunderscore 24\textunderscore 1935)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{1.0e-3} & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 190000

Time = 4300s

\vspace{2mm}

Standardized rewards, but learning rate multiplied by 40.

1.0e-2 is too much learning rate, it starts at the extremes. 5.0e-3 too. 2.5e-3 seems stable at the start but becomes unsettled on the first pass.

1.0e-3 seems to be the stable maximum, which is 4 times more than the previous one.

It doesn't seem to be as effective, in 100000 steps it doesn't catch the jumps well, even if the reward increases from -0.43 to -0.2

\section*{\color{green} 24/4 21:30 (4\textunderscore 24\textunderscore 2130)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{2.5e-4} & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 366000

Time = 8400s

\vspace{2mm}

Reward multiplied by 80, parameters of 4\textunderscore 24\textunderscore 1600. 

Similar results are obtained as when multiplied by 40.

In this case you get to 0 at 196000, and to -4 at about 150000 (practically the same proportion).

It would be advisable to adjust the learning curriculum again with these rewards (or those of *40) and check whether or not it is faster. Then add the movement memory again.

\section*{\color{green} 25/4 19:15 (4\textunderscore 25\textunderscore 1915)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 305000

Time = 7000s

\vspace{2mm}

Training as 4\textunderscore 24\textunderscore 1600 but with curriculum (adapted, TR from 0.5 to 0.01) It should approach the form in 80000 steps and have positive reward (in the last level) at 190000 to equal the training without curriculum.

In the end it seems to work similarly with less restrictive parameters, but as the minimum conditions are unbalanced it has not passed the first level (behaving almost the same). A next training will be done by setting the 0 of limits to overcome levels.

Probably the curriculum can be used to increase the restrictions, but this requires more observations.

The parameter of curiosity is of no use to me.

\section*{\color{red} 26/4 14:21 (4\textunderscore 26\textunderscore 1421)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 256 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 1\\
		\hline
			 summary freq & 1000 & time horizon & 5\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 & &\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

\textbf{Observation space size = 25}

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 573000

Time = 13200s

\vspace{2mm}

Training without curriculum, with the 25 added observations (from the agent). The change of direction has also been activated, to see if it learns the context.

The reward is stagnant over -10 from the 160000 steps. The graph shows how it oscillates over the center. For future models it may be necessary to increase the complexity of the training and the network. Another way is to make the network in control and the bot indicate how to act.

\section*{\color{green} 26/4 18:15 (4\textunderscore 26\textunderscore 1815)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{1024} & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{8192} & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{5} & \textbf{num layers} & \textbf{2}\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{256}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 763000

Time = 16750s

\vspace{2mm}

It should improve accuracy, but be much slower.

Idea: put as previous values of movement the average between the bot and the agent.

The graph recognizes impulses very well, but the idle oscillates between 0 with quite a lot of noise. This must be because the network doesn't remember the previous images, and doesn't distinguish when to change direction. The growth is much slower and it stagnates again on the same level of reward. This could be solved by pre-recording the image or simplifying the algorithm in cases where more than one target appears.

\section*{\color{red} 27/4 10:35 (4\textunderscore 27\textunderscore 1035)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 5 & num layers & 2\\
		\hline
			 summary freq & 1000 & time horizon & 256\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 132000

Time = 2900s

\vspace{2mm}

Improved bot so that it always chooses the closest target, at the moment the change of direction is removed. In this one we are going to use curriculum to see if it improves the learning speed.

The curriculum doesn't seem effective, after almost 3000s it has only reached -14 of reward (from the easy level).

\section*{\color{red} 27/4 11:23 (4\textunderscore 27\textunderscore 1123)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 5 & num layers & 2\\
		\hline
			 summary freq & 1000 & time horizon & 256\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 366000

Time = 8000s

\vspace{2mm}

Same model as before but without curriculum. Similar result, it stagnates at -10.

\section*{\color{red} 27/4 13:39 (4\textunderscore 27\textunderscore 1339)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			learning rate & 2.5e-4 & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 5 & num layers & 2\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{32}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 303000

Time = 6600s

\vspace{2mm}

Tests with different parameters: time horizon (from 256 to 32) does not seem to change anything.

\section*{\color{green} 27/4 15:33 (4\textunderscore 27\textunderscore 1533)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & \textbf{hidden units} & \textbf{64} \\
		\hline
			\textbf{learning rate} & \textbf{5.0e-4} & \textbf{learning rate schedule} & \textbf{constant}\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{3} & \textbf{num layers} & \textbf{3}\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{256}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 3000

Total steps = 942000

Time = 19900s

\vspace{2mm}

Different network architecture. It has more layers but fewer units per layer.

It's close to the shape but still has a lot of noise, plus it's very slow. The curiosity parameter may need to be retrieved and add more observations. After many hours it gets to -6 reward.

It may also be possible to reduce the length of each episode.

\section*{\color{red} 28/4 18:00 (4\textunderscore 28\textunderscore 1800)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & \textbf{hidden units} & \textbf{128} \\
		\hline
			\textbf{learning rate} & \textbf{4.0e-4} & learning rate schedule & constant\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & \textbf{num layers} & \textbf{2}\\
		\hline
			 summary freq & 1000 & time horizon & 256\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			\textbf{curiosity strength} & \textbf{0.05} & \textbf{curiosity gamma} & \textbf{0.99}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{256} &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

\textbf{Action space size = 2}

\vspace{2mm}

\textbf{Episode steps = 1000}

Total steps = 74000

Time = 1550s

\vspace{2mm}

Drastic changes in rewards (factors and standard deviations). Now returns 2 values in vector action (the expected maximum and minimum).

With this test I know that the model can set the maximum and minimum. Incomplete.

\section*{\color{red} 28/4 18:33 (4\textunderscore 28\textunderscore 1833)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 128 \\
		\hline
			learning rate & 4.0e-4 & learning rate schedule & constant\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 3 & num layers & 2\\
		\hline
			 summary freq & 1000 & time horizon & 256\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 360000

Time = 7600s

\vspace{2mm}

Same test as before, but longer. It manages to wrap the movement but with too wide a range.

\section*{\color{red} 29/4 10:34 (4\textunderscore 29\textunderscore 1034)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & \textbf{beta} & \textbf{1.0e-2} \\ 
		\hline
			buffer size & 8192 & \textbf{hidden units} & \textbf{256} \\
		\hline
			\textbf{learning rate} & \textbf{6.0e-4} & \textbf{learning rate schedule} & \textbf{linear}\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{10} & num layers & 2\\
		\hline
			 summary freq & 1000 & \textbf{time horizon} & \textbf{512}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 226000

Time = 5400s

\vspace{2mm}

The reward now takes into account the relationship between standard deviations.

Like the previous one, it stagnates at a certain point (this time at -1, but with similar behavior). 

\section*{\color{red} 29/4 12:09 (4\textunderscore 29\textunderscore 1209)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 1.0e-2 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{2.0e-5} & learning rate schedule & linear\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 10 & num layers & 2\\
		\hline
			 summary freq & 1000 & time horizon & 512\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 256 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 200000

Time = 4700s

\vspace{2mm}

Same training but with a lower learning rate (and scores multiplied by 20). Doesn't seem to advance too far so it's not saved.

\section*{\color{red} 29/4 13:34 (4\textunderscore 29\textunderscore 1334)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & \textbf{beta} & \textbf{5.0e-3} \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{5.0e-5} & \textbf{learning rate schedule} & \textbf{constant}\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 10 & num layers & 2\\
		\hline
			 summary freq & 1000 & time horizon & 512\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.8}\\
		\hline
			\textbf{curiosity strength} & \textbf{0.1} & \textbf{curiosity gamma} & \textbf{0.9}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{128} &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 297000

Time = 7920s

\vspace{2mm}

For some reason the 2 curves end up below the bot value, and it doesn't get much of a penalty. There must be some mistake in the rewards that allows him not to learn properly and still increase the reward.

\section*{\color{red} 29/4 18:27 (4\textunderscore 29\textunderscore 1827)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			\textbf{learning rate} & \textbf{5.0e-4} & learning rate schedule & constant\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{5} & num layers & 2\\
		\hline
			summary freq & 1000 & time horizon & 512\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			\textbf{curiosity strength} & \textbf{0.01} & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 265000

Time = 6200s

\vspace{2mm}

The reward rises to positive but stabilizes at +12 after 130,000 steps. It reaches a positive reward at 90000.

The network learns quickly to separate the maximum and minimum but with very wide ranges. It would be necessary to increase the requirement by penalizing more neutral scores (and perhaps vary how each reward factor affects it).

\section*{\color{red} 30/4 10:43 (4\textunderscore 30\textunderscore 1043)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			learning rate & 5.0e-4 & learning rate schedule & constant\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 5 & num layers & 2\\
		\hline
			summary freq & 1000 & time horizon & 512\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 317000

Time = 7440s

\vspace{2mm}

Training with curriculum. It has the parameter ``requirement'', which makes that when the reward reaches 0.5, 1 is subtracted from future rewards so that it must adjust the graph more. It has several levels until subtracting 25 points (it goes from 1 to 1).

There are several levels of the curriculum that go up too fast, they should be compressed into less. On the other hand, I have found that it is necessary to use a weighted average to make it react correctly to impulses.

\section*{\color{red} 30/4 13:16 (4\textunderscore 30\textunderscore 1316)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & hidden units & 256 \\
		\hline
			learning rate & 5.0e-4 & learning rate schedule & constant\\
		\hline
			max steps & 5.0e6 & normalize & false\\
		\hline
			num epoch & 5 & num layers & 2\\
		\hline
			summary freq & 1000 & time horizon & 512\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 265000

Time = 6215s

\vspace{2mm}

Same training but with several errors corrected (such as the relationship between stds or some score limits), in addition to having added a weighted mean and a minimum standard deviation. The learning curriculum levels should be adjusted later.

The network stagnates over 0, it does not get past the first level. It should learn to adjust the lines.

\section*{\color{red} 30/4 15:12 (4\textunderscore 30\textunderscore 1512)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & \textbf{epsilon} & \textbf{0.3} \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{1.0e-3}\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			\textbf{num layers} & \textbf{1} & summary freq & 1000\\
		\hline
			\textbf{time horizon} & \textbf{5} & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 128000

Time = 3050s

\vspace{2mm}

Tests with different parameters. It learns faster but it gets stuck again. This seems to have its cause in the fact that if the agent estimates a very large range in relation to the deviation of the bot even though the reward is decreased, the punishment is also decreased. To adjust this phenomenon I thought to change the deviation ratio parameter to another factor that will also depend on coherence (using clipped cubic functions).

It may be necessary to combine learning with behavioral cloning at first.

\section*{\color{red} 30/4 17:14 (4\textunderscore 30\textunderscore 1714)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 1.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 225000

Time = 5500s

\vspace{2mm}

Corrected the deviation ratio factor, now penalizes more or less depending on coherence (if by covering a large range fails, the punishment is increased, if it succeeds, the reward is decreased).

Apart from what was mentioned before of BC with GAIL, it would also be good to try a multiplicative (instead of subtractive) requirement once the rewards are balanced.
In spite of lowering the condition to move up a level, it does not converge but stays where it was.

\section*{\color{red} 30/4 18:49 (4\textunderscore 30\textunderscore 1849)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{2.0e-3}\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 225000

Time = 5500s

\vspace{2mm}

Same training as before but with a higher learning rate and greater initial demand.

Still not stabilized. It will be necessary to penalize the fact that the range is greater at the moment of stability (instead of decreasing reward).

\section*{\color{red} 01/5 10:05 (5\textunderscore 01\textunderscore 1005)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 90000

Time = 2100s

\vspace{2mm}

Training with an initial demand of 15. Compare with the previous ones to see where it stagnates.

Curiously, it is still stagnating at 0, but this one seems to be behaving a little better. Now I will test a training with much more demand, then the multiplicative demand and finally rewards that only penalize for moves being consistent and only reward for moves being inconsistent (more or less).

\section*{\color{red} 01/5 11:10 (5\textunderscore 01\textunderscore 1110)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 75000

Time = 1750s

\vspace{2mm}

Same training but with 50 demands. It may come closer to the target or divert it by being excessively negative. The graph converges to -50 so that the subtractive requirement has proven to be ineffective.

\section*{\color{red} 01/5 12:45 (5\textunderscore 01\textunderscore 1245)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 420000

Time = 10700s

\vspace{2mm}

Simpler training without a curriculum: if the movement is coherent it can only be penalized, if it is incoherent it can only be rewarded. At the moment I use linear formulas.

As the rewards are now, it slowly goes up to -7 and at 360000 steps it diverges. Halfway through it seemed to fit the coherent movement, but it doesn't capture the impulses well (it doesn't get enough reward).

\section*{\color{green} 01/5 15:52 (5\textunderscore 01\textunderscore 1552)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 5.0e6\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			\textbf{curiosity strength} & \textbf{0.1} & \textbf{curiosity gamma} & \textbf{0.8}\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 390000

Time = 10000s

\vspace{2mm}

Inconsistent movement reward improved by 10, and increased curiosity.

This time it has not collapsed, and although it adapts quite well to coherent movement it does not carry impulses very well (maybe because it is not penalized at all, although sometimes it ``feigns'' to follow them).

\section*{\color{green} 01/5 18:51 (5\textunderscore 01\textunderscore 1851)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & \textbf{beta} & \textbf{8.0e-3} \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			\textbf{learning rate schedule} & \textbf{linear} & \textbf{max steps} & \textbf{6.0e5}\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 15800s

\vspace{2mm}

Added small negative rewards when an impulse fails (less than for coherent movements). In addition, training has been limited to 600,000 steps with a linear learning rate.

The movement seems correct on the graph, although it would be nice to decrease the amount of noise. However, it is enough to use GAIL with correct rewards.

*Thinking about it, it would also be good to treat cases where the standard deviation is 0, since a model like this where the deviation has a certain minimum would not be good for behaviours where the bot can stand completely still. FURTHER, it would be convenient if the deviation could be decreased once it has reacted, to make the transitions between movements more fluid (having a wide random range, the movement can be somewhat chaotic).

\section*{\color{red} 03/5 17:58 (5\textunderscore 03\textunderscore 1758)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
			\textbf{gail strength} & \textbf{0.01} & \textbf{gail gamma} & \textbf{0.99}\\
		\hline
			\textbf{gail encoding size} & \textbf{128} & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/MyStdDevDemo.demo}

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 516000

Time = 15200s

\vspace{2mm}

Added agent demonstration and GAIL reward. Pretraining parameter could be added later to check if it improves training. The result is very similar to the previous one, and that is why it is not considered as good.

\section*{\color{red} 03/5 22:23 (5\textunderscore 03\textunderscore 2223)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
			\textbf{gail strength} & \textbf{0.6} & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & &\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/MyStdDevDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 280000

Time = 8000s

\vspace{2mm}

More weight to GAIL rewards. With 0.7 gamma, it diverts at 82k steps.

With the above parameters it has strange behaviors, like placing the 2 lines over 0. It may need pre-training or a larger dataset.

\section*{\color{red} 04/5 10:08 (5\textunderscore 04\textunderscore 1008)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			\textbf{pretraining strength} & \textbf{0.5} & \textbf{pretraining steps} & \textbf{10000}\\
		\hline
			\textbf{extrinsic strength} & \textbf{0.7} & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 &  & \\
		\hline
			gail strength & 0.6 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/MyStdDevDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 153000

Time = 4300s

\vspace{2mm}

At 40,000 steps it has received a great reward in curiosity that has caused the model to collapse, from there it has not recovered.

\section*{\color{red} 04/5 11:36 (5\textunderscore 04\textunderscore 1136)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 5 & &\\
		\hline
			pretraining strength & 0.5 & \textbf{pretraining steps} & \textbf{20000}\\
		\hline
			extrinsic strength & 0.7 & \textbf{extrinsic gamma} & \textbf{0.9}\\
		\hline
			\textbf{gail strength} & \textbf{0.5} & gail gamma & 0.99\\
		\hline
			\textbf{gail encoding size} & \textbf{256} & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/MyStdDevDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 16200s

\vspace{2mm}

Curiosity eliminated, changes in other parameters and more pretraining. Doesn't seem to get as close to the target as it used to, it stagnates earlier. For the next training, base rewards on gail only.

\section*{\color{red} 04/5 16:12 (5\textunderscore 04\textunderscore 1612)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{5.0e-4}\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			\textbf{time horizon} & \textbf{64} & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			\textbf{gail strength} & \textbf{1.0} & gail gamma & 0.99\\
		\hline
			\textbf{gail encoding size} & \textbf{128} & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/MyStdDevDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 393000

Time = 9400s

\vspace{2mm}

Rewards based on GAIL only. The learning rate has been lowered to stabilize learning.

No improvement at all. Maybe the problem is that the dataset is small.

\section*{\color{red} 05/5 10:58 (5\textunderscore 05\textunderscore 1058)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & \textbf{pretraining steps} & \textbf{40000}\\
		\hline
			gail strength & 1.0 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/LongStdDDemo.demo}

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 60000

Time = 1500s

\vspace{2mm}

Found a mistake that made the demos all wrong, the previous models do not serve as an example. Retraining with parameters similar to 5\textunderscore 04\textunderscore 1612 and a longer and more correct demo. In this one only GAIL is used to see the result.

At the beginning it has had a very high rise, but it has stagnated afterwards. As the adversary network was not going to train anymore, I don't expect it to go up much.
Set pretraining steps to 0 to keep the opponent net training. Maybe it can be turned off at the end. Also increase the strength.

\section*{\color{red} 05/5 11:58 (5\textunderscore 05\textunderscore 1158)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			\textbf{pretraining strength} & \textbf{0.7} & \textbf{pretraining steps} & \textbf{600000}\\
		\hline
			gail strength & 1.0 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 110000

Time = 2700s

\vspace{2mm}

The reward goes up but ends up swinging in negative values. It may be necessary to increase the learning rate, increase entropy regulation or add environmental rewards. It seems that the network manages to fool the opponent's network faster than it learns.

\section*{\color{red} 05/5 12:24 (5\textunderscore 05\textunderscore 1224)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{2.0e-3}\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			\textbf{pretraining strength} & \textbf{0.8} & pretraining steps & 600000\\
		\hline
			\textbf{extrinsic strength} & \textbf{1.0} & \textbf{extrinsic gamma} & \textbf{0.99}\\
		\hline
			\textbf{curiosity strength} & \textbf{0.1} & \textbf{curiosity gamma} & \textbf{0.99}\\
		\hline
			\textbf{curiosity encoding size} & \textbf{128} & & \\
		\hline
			gail strength & 1.0 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 50000

Time = 1300s

\vspace{2mm}

Standard rewards restored. Imitation has been given more strength but has less influence on the model's reward.

I have interrupted the model to train a reference one, with the values of the one that was successful but the new rewards to see if it is still correct. The model has managed to raise the rewards several steps, although I should analyze issues such as entropy or losses to adjust more parameters. The goal is to get a faster workout than the benchmark, so that it can be scaled.

\section*{\color{green} 05/5 13:00 (5\textunderscore 05\textunderscore 1300)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.8}\\
		\hline
			curiosity strength & 0.1 & \textbf{curiosity gamma} & \textbf{0.8}\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 14100s

\vspace{2mm}

Training with the previous values (except time horizon) of the one that came closest to success. Quite correct model, serves as a reference for rewards (training time should be improved). However, it doesn't handle impulse linking or the ones on the right side very well. From the demo a reward of 0.25 can be expected as a maximum optimistic (this would be quite correct behaviour).

\section*{\color{red} 05/5 17:15 (5\textunderscore 05\textunderscore 1715)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			\textbf{pretraining strength} & \textbf{0.5} & \textbf{pretraining steps} & \textbf{600000}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			\textbf{gail strength} & \textbf{0.02} & \textbf{gail gamma} & \textbf{0.99}\\
		\hline
			\textbf{gail encoding size} & \textbf{256} & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/LongStdDDemo.demo}

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 77000

Time = 2000s

\vspace{2mm}

Test with the 3 parameters (inspired by Pyramids), following the indications of the documentation. The aim is to achieve a faster or more rewarding workout than the reference one (5\textunderscore 05\textunderscore 1300).

It has started well but ends up being between -60 and -80. In the following I will add other optional parameters to see if the learning is stabilized.

\section*{\color{red} 05/5 17:52 (5\textunderscore 05\textunderscore 1752)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps &  600000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.02 & gail gamma & 0.99\\
		\hline
			gail encoding size & 256 & \textbf{gail use vail} & \textbf{true} \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 88000

Time = 2300s

\vspace{2mm}

Vail has been added to the training (bottleneck). It should be more stable, but, decrease curiosity, add use actions (to imitate, in theory) and/or add learning rate of GAIL.

Very similar result.

\section*{\color{red} 05/5 18:34 (5\textunderscore 05\textunderscore 1834)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps &  600000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.02 & gail gamma & 0.99\\
		\hline
			gail encoding size & 256 & \textbf{gail use vail} & \textbf{false} \\
		\hline
			\textbf{gail use actions} & \textbf{true} & &\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 110000

Time = 2900s

\vspace{2mm}

Added use actions to true, and removed the use vail.

It oscillates on the same values, although a little higher, but still not improving on the original.

\section*{\color{red} 05/5 19:25 (5\textunderscore 05\textunderscore 1925)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps &  600000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			\textbf{gail strength} & \textbf{0.01} & \textbf{gail gamma} & \textbf{0.95}\\
		\hline
			\textbf{gail learning rate} & \textbf{0.00005} &\textbf{gail encoding size} & \textbf{64}\\
		\hline
			\textbf{gail use vail} & \textbf{true} & gail use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 98000

Time = 2500s

\vspace{2mm}

Changes in a few variables, and added learning rate. It shows the same trend.

\section*{\color{green} 05/5 20:17 (5\textunderscore 05\textunderscore 2017)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.8\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.95\\
		\hline
			\textbf{gail learning rate} & \textbf{0.0005} & gail encoding size & 64\\
		\hline
			gail use vail & true & gail use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 15200s

\vspace{2mm}

Pretraining removed. The result was practically the same as in the reference, and has reached the same reward. However, it does not seem that the GAIL rewards have influenced anything, maybe the curiosity is too high, besides, it has taken 20 minutes more to do the same steps. It might be worth trying out new rewards.

\section*{\color{red} 06/5 10:36 (5\textunderscore 06\textunderscore 1036)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.9}\\
		\hline
			\textbf{curiosity strength} & \textbf{0.05} & \textbf{curiosity gamma} & \textbf{0.9}\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 527000

Time = 12500s

\vspace{2mm}

Benchmark training with the new linear rewards. A jump may have to be added to the penalties depending on the outcome.

The reward goes up in parallel to the previous reference, but it doesn't take much to detect impulses. It will be necessary to add a jump to the penalties.

\section*{\color{red} 06/5 14:15 (5\textunderscore 06\textunderscore 1415)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 242000

Time = 5200s

\vspace{2mm}

Added a jump in the rewards (locking everything up penalizes the same as failing by ~0.00001 units). It has a very similar trend, so it doesn't improve the previous one. 

\section*{\color{red} 06/5 15:45-18:48 (5\textunderscore 06\textunderscore 1848)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 8.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.3 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & false & num epoch & 5\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.9\\
		\hline
			curiosity strength & 0.05 & curiosity gamma & 0.9\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = -

Time = -

\vspace{2mm}

Parameter tests. None of them work as well as they should. The next test with SAC.

\section*{\color{red} 07/5 09:56 (5\textunderscore 07\textunderscore 0956)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 50000 \\ 
		\hline
			buffer init steps & 0 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 1.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 2 & time horizon & 1024\\
		\hline
			sequence length & 64 & summary freq & 1000\\
		\hline
			tau & 0.01 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.5 & pretraining steps & 10000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 72000

Time = 4600s

\vspace{2mm}

Training with SAC. It is much slower than PPO but requires less samples, has risen fast but has diverged by 70k.

\section*{\color{green} 07/5 11:42 (5\textunderscore 07\textunderscore 1142)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 50000 \\ 
		\hline
			buffer init steps & 0 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & \textbf{learning rate} & \textbf{8.0e-4}\\
		\hline
			\textbf{learning rate schedule} & \textbf{linear} & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 2 & \textbf{time horizon} & \textbf{64}\\
		\hline
			sequence length & 64 & summary freq & 1000\\
		\hline
			\textbf{tau} & \textbf{0.007} & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			\textbf{pretraining strength} & \textbf{0.7} & \textbf{pretraining steps} & \textbf{20000}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & \textbf{use actions} & \textbf{true}\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 194000

Time = 11900s

\vspace{2mm}

Back to previous rewards and changes in some parameters. The network adapts quite well to impulses although it has intermittent drops in rewards (smaller each time, it may be part of how the model works). Better results could be obtained with this model, but the training is much slower in proportion.

\section*{\color{red} 07/5 15:12 (5\textunderscore 07\textunderscore 1512)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 50000 \\ 
		\hline
			\textbf{buffer init steps} & \textbf{2000} & \textbf{hidden units} & \textbf{256} \\
		\hline
			\textbf{init entcoef} & \textbf{1.5} & \textbf{learning rate} & \textbf{1.0e-3}\\
		\hline
			\textbf{learning rate schedule} & \textbf{constant} & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{1} & time horizon & 64\\
		\hline
			sequence length & 64 & summary freq & 1000\\
		\hline
			\textbf{tau} & \textbf{0.005} & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.7 & pretraining steps & 20000\\
		\hline
			\textbf{extrinsic strength} & \textbf{2.0} & extrinsic gamma & 0.99\\
		\hline
			\textbf{curiosity strength} & \textbf{0.02} & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			\textbf{gail strength} & \textbf{0.02} & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = -

Time = -

\vspace{2mm}

Changes in layer structure. Cancelled.

\section*{\color{red} 07/5 17:30 (5\textunderscore 07\textunderscore 1730)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 50000 \\ 
		\hline
			buffer init steps & 2000 & hidden units & 256 \\
		\hline
			init entcoef & 1.5 & learning rate & 1.0e-3\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 64 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.7 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 2.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.02 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/LongStdDDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 25

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 215000

Time = 4600s

\vspace{2mm}

GPU training tests, I don't know if it really works or not

-The time scale of the training cannot be changed.

-Adding twice as many environments causes twice as many steps to be taken, but the curve increases by half as fast (more or less).

In this case, the agent ends up farther away from the reward. The next, train with less learning rate.

\section*{\color{red} 07/5 20:39 (5\textunderscore 07\textunderscore 2039)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & \textbf{buffer size} & \textbf{200000} \\ 
		\hline
			\textbf{buffer init steps} & \textbf{5000} & hidden units & 256 \\
		\hline
			\textbf{init entcoef} & \textbf{0.7} & \textbf{learning rate} & \textbf{4.0e-4}\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			\textbf{sequence length} & \textbf{128} & summary freq & 1000\\
		\hline
			tau & 0.005 & \textbf{use recurrent} & \textbf{true}\\
		\hline
			vis encode type & simple & & \\
		\hline
			\textbf{pretraining strength} & \textbf{0.4} & pretraining steps & 20000\\
		\hline
			\textbf{extrinsic strength} & \textbf{1.5} & extrinsic gamma & 0.99\\
		\hline
			\textbf{curiosity strength} & \textbf{0.03} & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			\textbf{gail strength} & \textbf{0.03} & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/NoObsDemo.demo}

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

\textbf{Observation space size = 0}

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 57000

Time = 3500s

\vspace{2mm}

The 25 previous observations have been changed to ``use recurrent''. The CNN encoder has also been changed, so it may take a little longer to train (it shouldn't be as relevant as before, when inferring by GPU).
It seems that increasing the buffer causes it to go much slower: contrast with the final learning result (if it doesn't improve, check the same without buffer, if it doesn't change it will be that the 25 observations are essential). It has not learned anything.

\section*{\color{green} 07/5 22:09 (5\textunderscore 07\textunderscore 2209)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			\textbf{init entcoef} & \textbf{1.0} & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & \textbf{use recurrent} & \textbf{false}\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/NoObsDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 14000s

\vspace{2mm}

Use recurrent is what causes training to slow down considerably.

Without memory parameters it still seems to adapt correctly, and has a speed similar to ppo. However, it performs a kind of ``counterpulses'', which perhaps would be corrected with more training time.

The rewards of each standard deviation could be separated (by giving 1 of each + the composite), and if noise were added, the actual previous steps could also be used (in SAC they may work better).

\section*{\color{green} 09/5 12:22 (5\textunderscore 09\textunderscore 1222)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/IndDemo.demo}

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 14000s

\vspace{2mm}

Individual rewards have been added for each line (they are currently only linear, and with less weight). The demo is changed so that the rewards are consistent (maximum is ~3). It is saved as a checkpoint for further training. It adapts surprisingly well to impulses, although I think there is still room for further noise reduction.

\section*{\color{red} 09/5 16:43 (5\textunderscore 09\textunderscore 1643)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/IndDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 14000s + 7700s

\vspace{2mm}

Same model as the last one but woth more training steps. Max steps has been changed to leave it indefinitely. It has not had any major performance improvements. 