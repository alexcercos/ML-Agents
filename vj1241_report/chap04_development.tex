% ==============================================================================
% This file is part of the "LaTeX template for writing the Final Degree Work
% report". It has been developed to aid the students of the Bachelor's Degree in
% Video Game Design and Development at the Jaume I University.
%
% (c) 2019 Sergio Barrachina Mir and José Vte. Martí Avilés
%
% The template can be used and distributed under the next license:
%  Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)
%  http://creativecommons.org/licenses/by-nc-sa/3.0/
%  http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode
%
% Atom editor configuration follows:
% !TEX root = ./report.tex
% !TeX spellcheck = en-US
% ==============================================================================

\chapter{Work Development and Results}

\minitoc{}

\bigskip{}

In this section, we will detail the development of the project starting from the game creation, and then describing each step of increased complexity of the behaviour to imitate, as well as the results obtained in each step. 

To avoid confusion, we will refer to the programmed behaviors that we want to imitate as ``bot'' or ``NPC'', and the generated neural networks that have to learn to imitate that bot will be called ``AI'' or ``agent''.

\section{Game Development}

The game environment needs to include the ML Agents' classes (Agent and Academy) to train and play the game using the network. The game structure is the one shown in section ~\ref{sec:system_design}.

The camera has an NPC and a trained AI attached. One of them controls its movement automatically, and they can be changed in play time. Both of these classes have getters to 3 variables that correspond to the possible movements: mouse X movement, mouse Y movement and mouse click, which are used to rotate and shoot.

The Spawner creates randomly and saves references of black planes in the scene, which correspond to the enemies.

To end with, a Debug Canvas has been added as interface, which draws lines with the movements made and the ones expected by the neural network. This allows to see how well the neural network is training.

\section{Reactive Behaviors}

The first human behavior we would analize is reactions, which can be defined as ``sudden changes produced by a stimulus''. To model this behavior, we created a Bot with the following requirements:

\begin{itemize}
 \item While not seeing any target, it moves to the left uniformly
 \item When a target enters the screen, it reacts moving fast towards its center, then continues moving as normal
\end{itemize}

At this first step, the bot will only move in the Y axis, and it will be considered that is always clicking (so the targets would be destroyed whenever the sight touches them).

\subsection{Training with Proximal Policy Optimization (PPO)}
\label{sec:trainingPPO}

Proximal Policy Optimization~\cite{ppopolicy} is the first and most simple reinforcement learning algorithm  provided by ML Agents~\cite{mlagents}. It uses a neural network to approximate the ideal function that maps an agent's observation to the best action it can take in a given state. Also, it is the fastest algorithm of all provided by ML Agents.

In the following subsections, some training related issues will be taken into account. At first, we will train our models using this policy (PPO).

\subsection{Unnecessary actions}
It is important not to add more actions than needed, since they would slow down the training process considerably. Even though it is possible to move in X and Y and perform clicks, since the bot only moves using the Y axis any additional action would add much noise to the AI. 

That is caused because when training the AI is overfitted with demonstrations with Y movements of exactly 0, and when that AI is playing any slight up or down movement would go inside untrained cases, and then causing unexpected behaviors.

Therefore, in this case the neural network would only have 1 action output: the X axis movement.

\subsection{Rewards based in tolerable range}
\label{sec:tolrange}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{img/rewTR.png}
		\caption{Rewards based in tolerable range.}
		\label{fig:trange}
\end{wrapfigure}

Our first reward approach is based in tolerable ranges. This consists in giving positive rewards when the distance between the guess and the real move is less than the tolerable range:

-The maximum reward is given (1) if it the distance is exactly 0

-A reward of -1 is given when the distance is 2, which is the maximum distance possible (NPC moving at maximum speed in one direction and the AI in the opposite direction).

In Figure~\ref{fig:trange}, you can see the reward function with tolerable range = 0.5. In our trainings, tolerable range was between 0.05 and 0.1: lower tolerable ranges caused the training to become unstable because it only got negative rewards, and higher tolerable range

Models trained using this rewards are not very time-efficient. If the tolerable range is too big (the agent receives positive rewards easily), the model doesn't fit the movement; if it is too small (receives negative rewards), the agent tends to stay only in the average movement, and doesn't react at all. That happens because the average is the point with biggest chance of reward (the agent is only punished when an impulse occurs). 

Curriculum learning~\footnote{Curriculum learning is a technique provided by ML Agents to train complex behaviors with consecutive lessons that increase in difficulty. That way, when the agent learns one task it goes on to the next lesson.} does not improve the training performance since with the initial less exigent punishments, the neural network learns much slower than with higher ones.

Figure~\ref{fig:graphTR} shows some of the success cases. From left to right, the first image shows how the neural network model adapts to the idle movement and the impulses, after 180000 training steps (4100s). The middle image displays an imperfect behavior of the same model when successive impulses occur. The right image is the same model trained longer time (10000s, 435000 steps), and how it tends to excessively smooth its impulsive movements. The causes of these two problems (successive impulses and smoothing) are discussed in section~\ref{sec:determ}.
\begin{figure}[h]
  \centering
		\includegraphics[width=.9\textwidth]{img/graphsTR.png}
  \caption{Bot movement (red) and neural network movement (green)}
  \label{fig:graphTR}
\end{figure}

\subsection{Determinism of the behavior}
\label{sec:determ}

Since at this point the neural network does not receive past events as input (neither moves or images), the movements performed by the bot have to be deterministic in order to train correctly: that is, given a frame, the bot would react with the exact same move every time (in the impulses, the default movement has a bit of noise in it). However, by how the bot was made it always took as objective the first image that it had seen, until destroyed.

In some special cases, when a new target spawns nearer to the sight than the current objective, the bot would not change the target order, and so it would behave differently depending on the context, as you can see in figure~\ref{fig:graphTR}. These repeated events cause the neural network to confuse when multiple targets are on screen, and if trained longer, it tends to do smaller impulses until only moving in the average move.

\begin{figure}[h]
  \centering
		\includegraphics[width=.9\textwidth]{img/exampleDet.png}
  \caption{2 situations that lead to different actions with the same frame}
  \label{fig:graphTR}
\end{figure}

This issue is solved by making the bot behavior deterministic or adding a movement memory.

\subsection{Movement memory}
\label{sec:movememory}

In order to prepare the bot to have reaction times, 25 previous moves distributed in the last 2 seconds are added as observations. What move is added as observation is critical.

If the real bot movement is added, the bot reaches high rewards very quickly but doesn't learn to imitate the bot: that's because the neural network learns to ``mimic'' the last move made by the bot, so it has high chance of reward with only one important observation. When playing the game with the trained neural network, it would not move (in the beginning, all the previous moves are 0) until it starts moving in one or another direction at maximum speed (See Figure~\ref{fig:graphMem}). This happens when the movement starts increasing in value due to impressions in the returned action of the neural network that make it believe that it is accelerating in movement.

When using the neural network movement, it learns like before: correctly but a bit slower. However, the previous moves tend to have noise at first, and the network could learn to ignore them.

\begin{figure}[h]
  \centering
		\includegraphics[width=.5\textwidth]{img/graphsMemory.png}
  \caption{A badly trained model while training (up) and playing (down)}
  \label{fig:graphMem}
\end{figure}

A better approximation would be interpolating the real move with the neural network's one: at the start, the movement added as observation in the next frames would be the NPC move. When the AI starts learning to adapt to the context (previous moves), the movement added would be an interpolation between the AI and the NPC movement (which would cause the AI to react in time), until the original AI moves are the ones added as observation. This can be made using curriculum learning: the lesson with least difficulty is the one where the AI receives past movements of the NPC as observations, and the hardest one where it receives its own movements as observation.

\subsection{Rewards based in standard deviation}
\label{sec:stdrew}

Since trained models using the methods explained in the previous sections tend to return the most common value, movements with more noise or imprecissions would not be produced correctly by the AI: when training, the AI could guess a move some units below the average of the previous moves but the NPC could have done a move the same units above the average, causing the network to be penalized, and causing the AI movement to converge to the average movement. To model these kind of noises more precisely, the actions and rewards should be changed.

In this section we propose a reward system based on standard deviations (Figure~\ref{fig:stdexample}): the relation between standard deviation, average and the actual move would determine how coherent is a move in a given context.
\begin{wrapfigure}{r}{0.2\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/graphSTD.png}
		\caption{Weighted average and standard deviations of an irregular movement}
		\label{fig:stdexample}
\end{wrapfigure}

The \emph{coherence} of a movement can be defined as how centered it is, in relation to the average. A movement with maximum coherence (1) would be the exact average, a movement at a standard deviation distance would have coherence 0, and movements outside of the standard deviations would be considered ``incoherent''. Then, default movement with noise would be coherent moves, and impulses would be incoherent.

To model the behavior, the agent would do 2 actions instead of one: a maximum and a minimum guess. The more precisely it encloses the real movement, the higher reward it gets; if it fails enclosing it, a punish is given.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{img/stdPrecision.png}
        \caption{Points by precision}
        \label{fig:prec}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{img/stdFactors.png}
        \caption{Reward and punishment factors by coherence}
        \label{fig:stdf}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{img/stdDevfactor.png}
        \caption{Deviation factor divided by Pow(2,(1-coherence)), with coherence=1}
        \label{fig:devf}
    \end{subfigure}
    \caption{Shape of the 3 parameters used for rewards}
		\label{fig:stdshapes}
\end{figure}

\begin{wrapfigure}{r}{0.2\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/graphsSTDrew.png}
		\caption{Trained model using SD rewards}
		\label{fig:stdrewards}
\end{wrapfigure}

Coherent moves give higher punishments if failing and smaller rewards, and incoherent moves (impulses) give high rewards. Given a maximum and minimum values (actions provided by the neural network), the real move, the average of the last 25 moves, its standard deviation and the coherence parameter explained in this section, the reward system follows these rules:

\begin{itemize}
 \item \emph{Coherence} is inversely proportional to the \emph{reward factor}, and directly proportional to the \emph{punish factor}: high coherence means lower rewards and higher punishes.
 \item A movement has higher \emph{precision} if it's centered between the maximum and minimum, and less precision if it's outside
 \item The \emph{precision} is relative to the difference between the maximum and minimum values
 \item The \emph{deviation factor} is calculated dividing the real standard deviation with the agent one (max - min)
 \item The \emph{deviation factor} is inversely proportional to the coherence
 \item All the values are clamped to avoid excessively high rewards/punishments or zero division errors
 \item The final reward is calculated multiplying \emph{precision * factor * deviation factor}
\end{itemize}

In figure~\ref{fig:stdshapes} you can see the shapes of each parameter functions, used to calculate the final reward.


In this first approach using maximum and minimum estimations, the network doesn't fit well the movement: it encloses large areas continuously (See Figure~\ref{fig:stdrewards}). That could happen because it receives less punishment by enclosing the coherent movement than by fitting and sometimes failing, and also receives rewards from incoherent movement. Thus, the neural network finds an equilibrium enclosing wide ranges to catch high rewards from incoherent moves, at the cost of getting fewer rewards from coherent moves (which were low by definition) and not exposing to any punishment from failing to encase coherent moves.

\subsection{Rewards based in movement coherence}
\label{sec:cohrew}

Since last reward system didn't make the agent learn correctly, we need to change the rewards in a way that it worries about adjusting to the predictable coherent movement while also worrying about not to miss any impulsive incoherent move.

Rewards based on movement coherence are a simplification of the reward system exposed on section ~\ref{sec:stdrew}, where coherent moves can only punish and incoherent moves can only give rewards. These motivates the agent to receive the least punishments by enclosing coherent moves, but also to take profit of potential rewards of incoherent moves. 

\begin{wrapfigure}[13]{rH}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{img/graphsCoherence.png}
		\caption{Coherence-based models with and different learning rate: left=2e-3, right=8e-3}
		\label{fig:precisionR}
\end{wrapfigure}

The punishments in coherent moves are calculated multiplying the punish factor, the coherence (0..1) and the relative distance between standard deviations, maximum and minimum.

The rewards in incoherent moves are calculated using the reward factor, the opposite to coherence (0..N, coherence is negative) and the precision factor shown in section~\ref{sec:stdrew}.

Models trained with this system adapt better to both coherent and incoherent moves, however they need high learning rate and at least 300000 steps to see acceptable results (see Figure~\ref{fig:precisionR}). However, a learning rate higher than 1e-2 can easily lead to unstable models that don't learn at all.

\subsection{PPO hyperparameters}

To sum up, the trained models that got decent performance had the following hyperparameters (they also depend on the reward system):

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 32 or 1024 & beta & 5.0e-3..8.0e-3\\ 
		\hline
			buffer size & 256 or 8196 & epsilon & 0.3\\
		\hline
			hidden units & mostly 256 & learning rate & 1.0e-4..2.0e-3\\
		\hline
			learning rate schedule & mostly linear & normalize & false\\
		\hline
			num layers & mostly 1 & num epoch & 3-5\\
		\hline
			summary freq & 1000 & time horizon & 5-256\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8..0.9\\
		\hline
			curiosity strength (opt.) & 0.01..0.1 & curiosity gamma (opt.) & 0.8..0.99\\
		\hline
			curiosity encoding size (opt.) & 128-256 & gail strength & 0.01 (not recommended)\\
		\hline
			gail gamma & 0.95 (not rec.) & gail learning rate & 0.0005 (not rec.)\\
		\hline
			gail encoding size & 64 (not rec.) & gail use vail & true (not rec.)\\
		\hline
			gail use actions & true (not rec.) & & \\
		\hline
	\end{tabular}
\end{center}

\subsection{Training with Soft-Actor Critic (SAC)}
\label{sec:trainingSAC}

Soft-Actor Critic~\cite{sacpolicy} is the second reinforcement learning policy provided in ML-Agents. It is characterized for being more sample-efficient and can learn from past experiences. However, it also executes slower, so the time needed to train a model is very similar both with PPO and SAC. Also, its training steps can be increased more easily since the learning rate is recommended to be constant (its Q function converges naturally).

To compare new methods with SAC and PPO, we've added simple linear rewards that affect the maximum and minimum individually, in addition to rewards based in movement coherence. These give reinforcement signals when one of the lines is well positioned, even when the cummulative reward is negative. In Figure~\ref{fig:pposac} you can see a cumulative reward comparison between an agent trained with SAC and other agent trained using PPO, with rewards based in coherence (see section~\ref{sec:cohrew}): SAC converges to a higher reward than PPO with much less steps.

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/PpoSac.png}
  \caption{Total rewards of SAC (pink) and PPO (green).}
  \label{fig:pposac}
\end{figure}

As final result, Figure~\ref{fig:graphPS} shows a comparison between both trained neural networks: SAC adapts much better to impulses than PPO, even though PPO also manages to fit the real move between the two lines. However, when playing neither of them reacts correctly to targets that appear on the right side (mainly because it is an uncommon case). After the training both models still have much noise in their default movement, but it could be corrected by training longer or by rewarding the stability of both agent lines (maximum and minimum).

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/graphSacPpo.png}
  \caption{Comparison between SAC (left) and PPO (right).}
  \label{fig:graphPS}
\end{figure}

Another aspect to take into account is how both methods can be applied using GPUs to boost the training process. SAC makes better use of the GPU: by training with 3 environments in parallel the training speed doubles (being equally fast as PPO using CPU) and also improves its efficiency. PPO training using GPU and 3 environments is almost 2.5 times faster than with CPU or GPU-SAC, but is more likely to produce an application crash than any other training method (because of GPU overheating or running out of memory).

\subsection{SAC hyperparameters}

These parameters were the ones used when training with SAC:
\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000\\ 
		\hline
			buffer init steps & 5000 & hidden units & 256\\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & pretraining strength & 0.4\\
		\hline
			pretraining steps & 20000 & extrinsic strength & 1.5\\
		\hline
			extrinsic gamma & 0.99 & curiosity strength & 0.03\\
		\hline
			curiosity gamma & 0.99 & curiosity encoding size & 128\\
		\hline
			gail strength & 0.03 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & use actions & true\\
		\hline
	\end{tabular}
\end{center}

%-----------------------------------------------
%-----------------------------------------------
%-----------------------------------------------

\pagebreak

\section{Reaction time}

In this section we will cover the development of bots with a behavior similar to the one presented in last section, but with delayed reactions: when the bot sees a target, it does not react instantly, but takes a few milliseconds to perform the action. This adds more complexity to the behavior and to the neural network, since it needs to receive information from previous frames (Moreover, the reaction time may not be exactly the same every time).

Even though the AIs with actions based in standard deviation (2 outputs to encapsulate a noised movement, see ~\ref{sec:stdrew}) did well modelling imprecise movements, we won't use this method in this section. That's because not only it would add more complexity to the task, but it could add much noise when moves are uncertain (if a bot reacts at 0.1-0.3 seconds, the AI would try to encapsulate a possible jump in all that range, and then if a random point in between was chosen as action each frame, the bot would not do a perfect impulsive movement. Instead, it would do a strange vibration). This feature will be solved with better reward systems (see section ~\ref{sec:rtrewsys}).

\subsection{Render Textures}

In order to provide the agent past observations, render textures must be used (Camera observations aren't useful in this context since they cannot provide past frames as input). ML Agents allows to provide multiple visual inputs (see ~\ref{fig:rendersens}), but they must follow these requirements:

\begin{itemize}
 \item Each render texture must have the same width and height~\footnote{This requirement also applies when only using one render texture (at least in this ML Agents version)}
 \item All render textures must be the same size
 \item All render textures must be either grayscale or not, but there must not be render textures of each type
 \item The minimum size is 20x20 pixels
 \item Each visual input must have an unique name (We use ``RenderTarget'' for the current frame and ``FrameXXX'' for past frames)
 \item Each visual input's render texture should not change in execution time~\footnote{Since there are multiple components of the same type, they cannot be changed reliably in real time}
\end{itemize}

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/rendersensor.png}
  \caption{Example of a Render Sensor component.}
  \label{fig:rendersens}
\end{figure}

With these restrictions, there are two reasonable methods to manage render textures in Unity:

The first method consists in creating a Render Texture array with the desired frames in the N-1 position of the array (last frame in position 0, the 5th frame before in position 4...). The current frame isn't included in the array since it is rendered directly from a virtual camera. The missing positions in the array must be filled with other Render Textures, even though the neural network would not receive them as input. Then, when a frame ends each render target copies the next frame (iterating the array backwards), until the current frame is rendered over the render texture at the position 0.
This method is not very optimal and has some errors at the start (render textures appear empty until frame N, being N the size of the render textures array) but allows to personalize easily which frames we want to provide as input to the neural network. Also, sometimes the copying of frames can overlap (frame N is being drawn on frame N+1, but before ending the process the frame N-1 starts being drawn over frame N).

Other more optimal and safe~\footnote{Even though this method is safer, it is important to ensure that the render textures that won't be used again are deleted. Not doing so will cause the memory to overflow, and the environment to stop without apparent errors.} method is using a list to store previous frames like a queue (see Figure ~\ref{fig:renderqueue} to visualize how it is executed): after each frame, a copy of the current rendered frame is saved at the start of the list, and the last is deleted if it exceeds the last frame provided as input. Then, for each frame that the network receives as input, the corresponding frame in the list is copied over it. With this method, each frame in the list isn't modified after being copied from the original.

\begin{figure}[h]
  \centering
		\includegraphics[width=.9\textwidth]{img/renderqueue.png}
  \caption{Render Queue: see that frame 2 isn't used as input for the neural network. Red arrows mean that the frame (or Render Texture) at the start is copied over the Render Texture at the end.}
  \label{fig:renderqueue}
\end{figure}

\subsection{Reward systems}
\label{sec:rtrewsys}

In this section, we've worked using 3 reward systems: tolerable range rewards, reward based in coherence and standard deviation and rewards after impulse.

\subsubsection{Tolerable range}

This reward system is the same that was described in section ~\ref{sec:tolrange}. With delayed reactions, this system is only effective if they are uniform: all reactions must occur at the same time. If not, the neural network would consider that is not worth the risk of doing an impulse if that had high punishments (See Figure ~\ref{fig:punDelay} to see an example of a high punishment when doing a correct impulse if the bot has non uniform reaction times).

With this method, the AI learned to do impulses correctly (even some that came from the right side, which is an exception case) but it didn't adapt well to the average movement: it had much more noise than the bot in default moves (See Figure ~\ref{fig:timeTR}).

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/timeTR.png}
  \caption{AI trained with tolerable range rewards. It adapts well to the impulses but not so much to the normal movement.}
  \label{fig:timeTR}
\end{figure}

\subsubsection{Rewards after impulse}

This reward system uses 2 queues, one for the AI and other for the bot. Whenever one of them does an impulse, instead of giving a score to the moves, they are stored in a queue. Then, when the other one does another impulse it is compared with the first's movement (either the AI or the bot can do the impulse first). If one of them did an impulse but the other didn't, after some time the AI would be penalized (either by missing an impulse or by doing impulses when it shouldn't).

The results obtained using this reward system were not very good: the agents didn't learn to do impulses at all. Since the rewards are given after both impulses are completed, the AI can get confused about when to do an impulse and how~\footnote{It is important to use large (0.99-0.995) gamma values for the rewards in this method: a big gamma parameter means that the agent looks for future rewards.}. Even though, this method is still the easiest way to model temporal noise, and with better balanced rewards it should perform well (See Figure ~\ref{fig:punDelay}).

\begin{figure}[h]
  \centering
		\includegraphics[width=.3\textwidth]{img/punDelay.png}
  \caption{Case when the AI would receive a double punishment when doing an impulse: if not using 2 queues, the agent would have a big punish even though the impulse was correct (the bot can react at either moment)}
  \label{fig:punDelay}
\end{figure}

\subsubsection{Rewards based in coherence and standard deviation}
\label{sec:rewchstd}

Like Tolerable Range rewards, this method has been applied to uniform reaction times. In our test cases, we have used reaction times between 1 and 8 frames of difference, and used as input for the AI the last 10 consecutive frames. This method is similar to the one explained in section ~\ref{sec:cohrew}, but applied to one action instead of two (the expected move instead the maximum-minimum guesses).

This method differentiates between coherent and incoherent moves: when the bot's movement is coherent (it is inside the average$\pm$standard deviation range), the AI receives a consistent reward (either a constant or 0) if its move is also outside that range, if not, it receives a punish that gets higher when the relative distance to that range increases. When the bot's movement is incoherent (impulse) there are 3 options:

\begin{wrapfigure}{rH}{0.2\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/graphRTstd.png}
		\caption{Model trained with 0.1 seconds of reaction time using this method}
		\label{fig:graphRTstd}
\end{wrapfigure}

\begin{itemize}
 \item If the AI move is closer to the bot's move than to the average, it receives a high reward (higher when closer)
 \item If the AI move is closer to the average move but between the average and the movement made, it doesn't receive any reward or punish
 \item If the AI move is in the opposite direction, it receives a punish
\end{itemize}

Using this method we have obtained the better results until this point (see Figure ~\ref{fig:graphRTstd}), still, it has more problems imitating the exception cases (for example, when a target appears at the opposite side). 

The quality of this results is very dependent on the precision of the average and the standard deviation. In section ~\ref{sec:dynamicstd} we explain how both of them were improved to get better results.

\subsubsection{Dynamic average and standard deviation}
\label{sec:dynamicstd}

At this point, to calculate the average and the standard deviation we were considering the last 60 moves. Each frame, the last move was deleted, the new one added to the list; then both the average and the standard deviation were updated. Using this amount of values was correct in some cases, but when some consecutive impulses happened they lost precision (see Figure~\ref{fig:stdVariations}), thus spoiling the reward system.

To add precision more values are needed, but too much values would be highly inefficient. To solve both of these problems, we use dynamic averages and dynamic standard deviations. Dynamic parameters are calculated using available previous information to avoid recalculating both values each frame: instead, when adding a new value, we use the last average (and standard deviation) and incorporate the new value to obtain the new average (or standard deviation). Both of them have O(1) computational cost (each frame) instead of O(N).

The dynamic average formula is the following:

\vspace{5mm}

$ {\displaystyle a_{n+1}=\frac{x_{n+1}+n\cdot a_n}{n+1}} $\\\\

Where we obtain the average for the next frame ($a_{n+1}$) from the last average ($a_{n}$), the new move value ($x_{n+1}$) and the amount of moves that have been used to calculate $a_{n}$ (n). Even though the formula can be obtained intuitively, the calculations used are explained at Appendix~\ref{app:average}.

\vspace{5mm}

The calculations needed to obtain the standard deviation $\sigma_{n+1}$ of the next frame require the last standard deviation $\sigma_{n}$, the new and old averages ($a_{n}$, $a_{n+1}$), the new move value ($x_{n+1}$), the amount of moves (n), and the average of all the moves squared ($\frac{\sum_{i=1}^{n}x_i^2}{n}$). This last parameter can be easily tracked using the dynamic averages explained above. 

The formula (see Appendix~\ref{app:stddyn}) for the dynamic standard deviation~\footnote{In the formula, the variance is used instead of the standard deviation to simplify the calculations, but we use only the standard deviation value} is the following:

\vspace{5mm}

$ {\displaystyle v_{n+1} = v_n + a_n^2-a_{n+1}^2 - \frac{\frac{\sum_{i=1}^{n}x_i^2}{n} - x_{n+1}^2}{n+1} } $\\\\

\noindent
The standard deviation is obtained taking the square root of the variance (v): $\sigma_n=\sqrt{v_n}$

Even though these two formulas stabilize both values, the standard deviation fails enclosing the noise of the coherent movement (it should be smaller) when there is a relatively big amount of impulses. To adapt better to the coherent movement, we interpolate the values of the moves outside of the standard deviation range~\footnote{Smoothing values may not be statistically correct for a standard deviation, but since the objective is to differentiate between Coherent and incoherent moves it is valid for our purpose}. The best interpolation parameter for the real move and its closer standard deviation was 0.4 (0.4·move + 0.6·standard deviation). Smaller interpolation parameters still made the range too big, and bigger interpolation parameters caused the standard deviation to increase too slowly~\footnote{The ideal interpolation parameter is approximately 0.4, however other parameters could work better with different amounts of noise. Still, this value works well in most of the cases.} (a parameter of 1 would cause the standard deviation to stay at value 0). See Figure~\ref{fig:stdVariations} to view a comparation between smoothed dynamic standard deviation, pure dynamic standard deviation and the non dynamic one.

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/stdVariations.png}
  \caption{Standard variations and average of the same bot using last 60 values (left), dynamic values (mid) and dynamic values smoothed by a 0.6 interpolation (right)}
  \label{fig:stdVariations}
\end{figure}

\subsection{PPO vs. SAC}

As we said in section~\ref{sec:rewchstd}, it is possible to model a correct behavior using a reward system based in standard deviations and movement coherence. However, it is important to clarify that all of those good results were obtained using PPO.

Even though SAC was more effective modeling non delayed reactive behaviors (see~\ref{sec:trainingSAC}), PPO performed better with delayed reactions. This could be caused because how both algorithms work and because unbalanced rewards:

PPO tends to optimize the agent to have the highest rewards in each situation, but SAC optimizes it to have an overall higher reward. Agents trained with SAC tend to the average move, not doing any impulse. PPO follows the impulses since approaching them gives a potentially higher reward in that situation. Still, SAC models had better scores just by not exposing themselves to the punishments of failing coherent moves (which were higher than the punishments of failing an impulse). In Figure ~\ref{fig:rtsacppo} you can see a comparison between both methods using the same reward conditions.

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/rtSACvsPPO.png}
  \caption{Comparison between an agent trained with SAC (left) and other trained using PPO (right). The bots in both cases have a reaction time of 0.1 seconds and 0.15 units of noise}
  \label{fig:rtsacppo}
\end{figure}

At this point, even though PPO performs better, both methods still fail at performing some impulses, when targets appear at the right side, or when two consecutive targets appear at the same time, and they don't adapt to noised reaction times (at least with the reward system exposed until this section).

\subsection{Behavioral cloning}

\begin{wrapfigure}{rH}{0.3\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/bcTNoise.png}
		\caption{Model trained with BC in 2000 steps. The bot's impulses appear deformed because the agent is the one playing the game}
		\label{fig:bcTNoise}
\end{wrapfigure}


Behavioral Cloning is the simplest algorithm provided by ML Agents: it directly copies the actions given in a demo (which can be recorded in the editor). However, it has its limitations: since it doesn't depend on environment rewards, the programmer cannot modify its behavior with reinforcement learning. Also, depending on the task, agents trained using these methods can have chaotic behaviors.

\begin{wrapfigure}[17]{rH}{0.3\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/bcTNovertrain.png}
		\caption{Model trained with BC in 7000 steps}
		\label{fig:bcTNovertrain}
\end{wrapfigure}

When trained with simple cases (in section~\ref{sec:spawner}, we treat how the rare cases were suppressed from the training), they perform really well. Also, the agents can model temporal noise effectively. The agent in Figure~\ref{fig:bcTNoise} is a simple case at the extreme: it receives the last 9 frames (and the current) as input and the bot can perform an impulse at each one of those frames (from 0.01s to 0.15s). The agent usually does the impulse at the average reaction time of the bot. It is worth noting that when the agent performs more or less correctly it's better to stop training, else it usually loses precision (see Figure~\ref{fig:bcTNovertrain}).

In the next 3 subsections, we will explain some problems that appeared when using behavioral cloning, and how they were solved.

\subsection{Recurrent memory} %lo de que se desvia por el input

Recurrent neural network are a feature that allow agents to have memory and remember past observations. They have the advantage of being optimized to ``choose'' what to remember, at the cost of giving less control to the user. Also, its training is much slower, and they have worse performance when infering.

A combination of past render targets, past moves and recurrent memory can obtain good models (even in some rare cases), but the resulting neural network is so heavy that the frame rate drops from roughly 70 fps to 25 fps. Even though it can perform most of the impulses, it usually has some strange artifacts (extra impulses) in its behavior (See Figure ~\ref{fig:recb}).

One problem~\footnote{According to the ML Agents documentation, recurrent memory is not recommended for continuous action spaces, which we are using} that appears when using recurrent networks with simpler inputs is that they are not foolproof: whenever they receive an unexpected input (in most of its inputs), its behavior can become chaotic. In figure~\ref{fig:recc} you can see an example of this problem: the neural network appeared to have trained well, but when it received an empty input for the past moves (at the start, all previous moves are 0), when they were returned by the recurrent memory, the agent became chaotic.

\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.3\textwidth}
        \includegraphics[width=\textwidth]{img/recurrentBrute.png}
        \caption{20 visual inputs and 25 past moves}
        \label{fig:recb}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[h]{0.3\textwidth}
        \includegraphics[width=\textwidth]{img/recurrentCaos.png}
        \caption{1 visual input and 25 past moves}
        \label{fig:recc}
    \end{subfigure}
		\caption{Agents trained with recurrent memory and behavioral cloning}
\end{figure}

In conclusion, recurrent neural networks are not recommended for this problem: behavioral cloning can adapt to the movement without them, and the training sessions and performance become much slower.

\subsection{Rare situations} %spawner
\label{sec:spawner}

As the game was programmed, targets spawn randomly at the stage. Since they can appear at any point, this provokes some situations that happen rarely: for instance, with the bots we have been using most targets appear by entering the screen in the left side (the bot moves continuously in that direction). However, some targets can spawn at its right side in a way that they can be seen, this happens approximately 1 in 12 times (the bot has a field of view of almost 60º). In figure~\ref{fig:spawncases} you can see some examples of some types of situations that appear in game.

\begin{figure}[h]
  \centering
		\includegraphics[width=.6\textwidth]{img/spawncases.png}
  \caption{Some of the different situations the neural network can encounter in the game}
  \label{fig:spawncases}
\end{figure}

Those rarer cases often suppose a harder task to solve by the neural network, but with temporal delays sometimes they cause the problem to be too complex to be approximated by the neural network (see Section~\ref{sec:complex}). Also, when they appear less times, they are learned slowly or not at all.

To improve the performance of the network, we can generate some rare cases in purpose: instead of spawning targets randomly, they can be spawned in the right side of the field of view, or two targets can be spawned next to each other, etc. Then, how often each case occurs can be adjusted manually.

This method was intended to make the agents learn faster, however it served to prove that the structure of the neural network was not enough to solve this problem.

\subsection{Complexity of the task} 
\label{sec:complex}

\begin{wrapfigure}{rH}{0.25\textwidth}
    \centering
    \includegraphics[width=0.2\textwidth]{img/complexity.png}
		\caption{Neural network that has diverged}
		\label{fig:complexity}
\end{wrapfigure}

Sometimes a task is too complex to solve by one neural network structure, with behavioral cloning is easy to detect this problem~\cite{xor}. These problems need more internal layers (and more training time) to be solved. For instance, the problem developed in this section (delayed reactions) needed at least 3 layers (with any policy) to be solved. When spawning rare cases more often, some agents that usually didn't learn the exceptions didn't also learn the common cases: the complexity to perform correctly each case was too high for 3 layers.


When using behavioral cloning this phenomenon occurs like this: the neural network starts adapting to the most common cases (but fails the least common), then the more time it is left training, the worse starts doing the common cases until the model diverges (Figure~\ref{fig:complexity}) and it starts showing strange behaviors. Sometimes it may cycle around all the process, but it would never learn.

