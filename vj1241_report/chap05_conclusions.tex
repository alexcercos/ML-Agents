% ==============================================================================
% This file is part of the "LaTeX template for writing the Final Degree Work
% report". It has been developed to aid the students of the Bachelor's Degree in
% Video Game Design and Development at the Jaume I University.
%
% (c) 2019 Sergio Barrachina Mir and José Vte. Martí Avilés
%
% The template can be used and distributed under the next license:
%  Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)
%  http://creativecommons.org/licenses/by-nc-sa/3.0/
%  http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode
%
% Atom editor configuration follows:
% !TEX root = ./report.tex
% !TeX spellcheck = en-US
% ==============================================================================

\chapter{Conclusions and Future Work}

\minitoc{}

\bigskip{}

\section{Conclusions}

In this document, different methods have been presented that allow the creation of agents through reinforcement learning and behavioral cloning that imitate NPCs with simple behaviors. However, these methods could not be extended to more complex cases successfully (non-functional requirement).

Each of the behaviors that have been studied have had very different results depending on the algorithm provided:
\begin{itemize}
	\item Instantaneous reactive movements can be replicated using any of the algorithms provided~\footnote{Behavioral cloning was not used for this task since it would be trivial for it to solve, however it was applied to delayed reactions successfully (instantaneous reactions can be considered as a sub-case of delayed reactions, with a reaction time of 0)}. In the case of the reinforcement learning algorithms is also necessary a reward system that is usually more complex than the movement itself. Within the 2 algorithms of this type that are available in ML Agents, SAC usually obtains better results than PPO.
	\item In the movements with delayed reactions, the reinforcement learning algorithms have greater problems to understand the problem, except in the cases in which the variability of the reaction times is null or practically null. This is because receiving rewards later than expected can confuse the agent. In cases where the variability is large, behavioral cloning is usually more successful if the dataset provided is good enough.
	\item For discrete actions, we took advantage of the possibility of obtaining information directly from the bot in order to model its behavior more effectively.In this type of actions, reinforcement learning algorithms clearly outperform the imitation learning ones (BC). Also, in our trainings related to discrete actions, continuous action spaces worked better than discrete action spaces.
	\item In our attempt to create more complex movements we trained agents to imitate the shape of a 2-axis movement. However, the agents had difficulties to imitate the bot with all the available algorithms, and it was not possible to get a good result in an agent that received its own previous movements as observations.
\end{itemize}

With these results, on the basis of the functional requirements provided in section~\ref{sec:funcReq}:

\begin{itemize}
	\item Some of the neural networks were able to play the game independently under specific conditions, and hardly ever under any situation.
	\item The network can receive as input and interpret what the camera is seeing
	\item The network is able to receive multiple past frames and actions. However, the more inputs it receives the more easily it can become unstable.
	\item The network can output one or multiple actions that the player can make
	\item The network can adapt its actions to reaction times of the NPC (but only if it receives enough past frames). Behavioral cloning allows to model reaction times that have more variability.
	\item Under some circumstances the network and the NPC can hardly be differentiated, however, when being exposed to rare or untrained situations the network does not act correctly.
\end{itemize}

The accomplishment of the non-functional requirements depends on the algorithms used and the complexity of the task: behavioral cloning is very sample efficient, and most of the networks that use reinforcement learning algorithms can usually be trained effectively in reasonable time (less than 1 hour). However, this framework is very difficult to scale to imitate a little more complex than those studied in this document.

%As preliminary conclusions, it has been proved that SAC policy is better than PPO to solve our task since it requires smaller datasets (or steps) and develops better behaviors than PPO (even though, PPO is better for testing since it is faster).

%ML Agents makes easier the development of neural network and its inclusion in 3D environments, however it still has some errors that complicate this process. Moreover, its policies are not completely optimized for GPU usage, and some simple convolutions slow the training process heavily. Also, the latest versions of CUDA and tensorflow are not supported.

%To end with, being able to design a custom neural network could improve the imitation results. It may be possible to do with ML Agents, but it could also be dangerous since it is necessary to modify source code in python. However, at this point we have already reached some of the milestones initially proposed by modeling reactive movements properly.

\section{Future work}

%This framework could be continued in more complex games, using real player data to model its behaviors. However, I don't plan on doing it in the near future since the computing needed would be very high and it should be trained outside of Unity (and incorporated into real games).

ML Agents is designed to develop simpler behaviors than imitating NPCs (and, of course, simpler than imitating humans).

On the other hand, ML Agents trainings are executed in the editor in real time, which not only decreases the quality of the obtained results but also exposes to execution errors or memory overflows (especially in cases where agents receive multiple previous input frames).

In most cases, reinforcement learning algorithms are ineffective for complex imitations, since the complexity of the reward systems increases much faster than the complexity of the behavior. The only exception is when there is access to both past and future information. This can be caused by imitating a bot whose actions can be determined sufficiently in advance or by having the information as a separate dataset. In the second case, there is no reason to perform real-time training as it is the case.

As future work, it may be possible to obtain better results with different network structures trained offline, such as classification neural networks~\cite{clasnn}, generative adversarial networks~\cite{radford2015unsupervised} or recurrent networks~\cite{zaremba2014recurrent}. However, they should be reincorporated to a game engine to be executed, and they would need to perform in real time.


\section{Final considerations}

Because we have used ML Agents to develop this work, there are sections of the planning that did not need to be done, or that could not be done:

\begin{itemize}
	\item Since the training sessions were executed in the editor, there was no need to create and save datasets~\footnote{ML Agents allows to create datasets to be used in imitation learning, however its content cannot be seen and they cannot be extended}.
	\item In ML Agents it's not possible to create custom neural networks without modifying its source code, which can be very risky. Configuration files allow to determine the number of layers and the number of hidden units in each layer. It can also apply convolutions to image inputs, but only individually. 
	\item Since we were unable to obtain reliable agents after increasing the complexity of the behavior, the framework was not standardized.
\end{itemize}

To end with, the project can be accessed and downloaded at the following link: \url{https://github.com/alexcercos/ML-Agents}

The instructions to open and execute the project can be found at Appendix~\ref{app:instructions}.

