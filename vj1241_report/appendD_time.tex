% ==============================================================================
% This file is part of the "LaTeX template for writing the Final Degree Work
% report". It has been developed to aid the students of the Bachelor's Degree in
% Video Game Design and Development at the Jaume I University.
%
% (c) 2019 Sergio Barrachina Mir and José Vte. Martí Avilés
%
% The template can be used and distributed under the next license:
%  Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA)
%  http://creativecommons.org/licenses/by-nc-sa/3.0/
%  http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode
%
% Atom editor configuration follows:
% !TEX root = ./report.tex
% !TeX spellcheck = en-US
% ==============================================================================
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

\chapter{Reaction time}
\label{app:time}

This appendix contains some notes that were taken when (and after) training sessions related to modeling delayed reactive behaviors, or in other words, agents with reaction time.

Any parameter that does not appear in one training is set to default (see ./config/trainer.config file).

Trained models with green titles are considered good or any improvement in the investigation. Models with red titles are considered failures.

Some of the models have not been saved, either because they don't perform well or they perform in much the same way as another model (agent).

Most of the notes should not be taken literally or as certain, as they usually are theories or preliminary conclusions drawn during the training itself.

\section*{\color{green} 09/5 18:55 (5\textunderscore 09\textunderscore 1855)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.8 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/IndDemo.demo

\vspace{2mm}

Render Target Sensor: 64x64

Grayscale: false

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 524000

Time = 5580s

\vspace{2mm}

Speed test using ppo with 3 envs and GPU, same rewards as in 5\textunderscore 09\textunderscore 1643

It works twice as fast, so it can be good for testing (and using SAC for final versions). However, it often gives errors like Out of memory (and other stranger ones that may be caused by overheating).

\section*{\color{green} 12/5 20:00 (5\textunderscore 12\textunderscore 2000)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Frames~\footnote{Frame 0 is the current frame, which is used in every training, the other frames are the n-th previous frame. }: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 37, 40, 45, 50, 55, 60, 70, 80}

\vspace{2mm}

\textbf{Render Target Sensor: 21 * (20x20)}

\textbf{Grayscale: true}

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 278000

Time = 17650s

\vspace{2mm}

Bot with 20 frames of memory (up to 1.4 seconds), and motion memory. No reactions are used, it serves to check efficiency with SAC and 3x envs.
At the moment it gives negative dimension error when making convolution.

Using 2 images of 16x16 fails, but with 2 of 64x64 it works. This last one also goes with grayscale.

With 3 16x16 images grayscale doesn't work. 16 of 64x64 fails.

With 3 images 64x64 grayscale does work. With 4 images it also works.

20x20 seems to be the minimum (or close) to be able to convolute without errors. However, it is necessary that all images are the same (I could check this later, but it seems to be the case). It gives error later. THIS ERROR was due to the fact that the demo was made with 16x16 render targets, so I remove GAIL at this moment.

Personally, I don't like very much the "21x20x20" thing.

The training becomes about 3 times slower despite using grayscale, but the results are not bad. It might be better to use PPO in these test cases.

GAIL may be quite relevant, as it has improved the reward but has not adapted to the idle movement too closely.

\section*{\color{red} 13/5 11:18 (5\textunderscore 13\textunderscore 1118)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			\textbf{pretraining strength} & \textbf{0.4} & \textbf{pretraining steps} & \textbf{20000}\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			\textbf{gail strength} & \textbf{0.03} & \textbf{gail gamma} & \textbf{0.99} \\
		\hline
			\textbf{gail encoding size} & \textbf{128} & \textbf{gail use actions} & \textbf{true} \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/Frames20Demo.demo}

\textbf{Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80}

\vspace{2mm}

\textbf{Render Target Sensor: 20 * (20x20)}

Grayscale: true

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 254000

Time = 20750s

\vspace{2mm}

Removed the frame 37. Training with a bot with noise and delay (BotDelayNoised, before it was BotNoised). Now the impulses have a different shape.

The curve is similar to the previous training. GAIL makes it even slower (from 3h 20m to 4h 20m at 200k steps). The average and deviations are better being more invariant, I should restore the previous settings (and improve the demo bot for this model).

\section*{\color{red} 13/5 18:19 (5\textunderscore 13\textunderscore 1819)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/Frames20Demo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 308000

Time = 25000s

\vspace{2mm}

Improved the trajectory of the averages, also in the demo.

The training is slow but with better proportion than the previous one (probably because the demo occupies less space). It manages to learn certain things but it is necessary to refine more how it adapts to the movements (both impulses and coherent).

\section*{\color{green} 14/5 09:52 (5\textunderscore 14\textunderscore 0952)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			\textbf{pretraining strength} & \textbf{0.5} & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/Frames20Demo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 0

Action space size = 2

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 18450s

\vspace{2mm}

It greatly improves the reward curve, perhaps because of the simplification of the frames (individually). Adapts to various reactions at the right time but has too much noise. It may be better to change the reward system by adapting it to the time dimension and have the agent take only one action again (and not discount it by how far it is, but by the average). It also greatly improves the SAC times, although the latter could have better results in the very long term. It is considered as good but the method will be changed. On the other hand, it should be noted that this model does not handle well the targets that appear on the right side.

After changing back to the previous 1 action method, train the model with pure behavioral cloning.

\section*{\color{red} 14/5 20:10 (5\textunderscore 14\textunderscore 2010)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/OneLineMem.demo}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

\textbf{Observation space size = 25}

\textbf{Action space size = 1}

\vspace{2mm}

Episode steps = 1000

Total steps = 378000

Time = 12000s

\vspace{2mm}

Simple training based on tolerable range of 0.05. 

As expected, it works the same as before: it adapts to the average but does not take impulses. The reward should be modified to accommodate the time difference (and the parameter of future rewards).

\section*{\color{red} 15/5 10:19 (5\textunderscore 15\textunderscore 1019)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 3.0e-4 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 9000

Time = 3200s

\vspace{2mm}

Behavioral cloning training. It begins with very good rewards but the model becomes unbalanced after a few iterations. Well trained one could say that it needs very few steps, but it is very slow in number of steps per minute.

\section*{\color{red} 15/5 11:08 (5\textunderscore 15\textunderscore 1108)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			\textbf{hidden units} & \textbf{256} & \textbf{learning rate} & \textbf{5.0e-5} \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			\textbf{num layers} & \textbf{1} & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 16000

Time = 5000s

\vspace{2mm}

Changes in architecture and learning rate. The reward decreases but more slowly. Next, train at a lower learning rate and with the previous layers.

\section*{\color{green} 15/5 12:37 (5\textunderscore 15\textunderscore 1237)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			\textbf{hidden units} & \textbf{128} & \textbf{learning rate} & \textbf{1.0e-5} \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			\textbf{num layers} & \textbf{2} & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 30000

Time = 10500s

\vspace{2mm}

Training with even less learning rate. I have noticed that in the beginning he does not do impulses anymore, it would be good to record some of the previous models at the beginning of the training to illustrate the memory (when he takes impulses).

It is considered good but still fails on lateral impulses.

\section*{\color{green} 15/5 17:03 (5\textunderscore 15\textunderscore 1703)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 25000

Time = 8700s

\vspace{2mm}

Training with Online BC. The results are slightly better, since they do not depend on an external dataset but on live action. Training times are equivalent. It still takes a long time to learn the impulses on the right side.

\section*{\color{red} 15/5 19:32 (5\textunderscore 15\textunderscore 1932)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 36000

Time = 12000s

\vspace{2mm}

Online training with the learning rate of 5\textunderscore 15\textunderscore 1237. It follows the same curve and ends up being chaotic.

\section*{\color{red} 21/5 10:08 (5\textunderscore 21\textunderscore 1008)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/QueueDemo.demo}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 180000

Time = 5900s

\vspace{2mm}

Training with a movement queue. It is important that the parameter of future rewards (gamma) is high. Training may be flawed by how the average and deviation is calculated right now.

Goes up slowly. It has failed at 180000 steps.

\section*{\color{red} 21/5 12:42 (5\textunderscore 21\textunderscore 1242)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 493000

Time = 15700s

\vspace{2mm}

Changes: normal movement does not score, any failed or overdone impulses discount, and successful impulses add up (more than failures).

Similar to previous models trained with PPO, it tends to average out but does not learn the impulses. 

The score could be improved if impulses could be treated separately (or movements in general).

\section*{\color{red} 21/5 17:14 (5\textunderscore 21\textunderscore 1714)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 254000

Time = 21000s

\vspace{2mm}

Training with SAC, in the same build as the previous one. Factors may need to be increased as it rarely receives rewards.

The graph converges quite well to the average, retaining some noise, but not reacting to any impulse. The reward is -0.34. Curiously, almost all of the score increase has been between 190-200k. 

Penalties need to be given much more weight, and perhaps even more weight to the extra impulses from the net.

\section*{\color{red} 22/5 10:45 (5\textunderscore 22\textunderscore 1045)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 255000

Time = 20750s

\vspace{2mm}

Rewards multiplied.

The rewards follow strange patterns, and the movement forms a kind of frequency. I think what it does is it accumulates little impulses to add up to a real one. I should better redefine how to score the impulses and what is considered impulse or not (and the standard deviation should be increased or made fixed).

In the end it stabilizes at the average, like most previous models.

\section*{\color{red} 23/5 10:36 (5\textunderscore 23\textunderscore 1036)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 320000

Time = 27500s

\vspace{2mm}

Changes in the reward, now eliminates whole impulses if it fails at first. It doesn't react to impulses after many hours. He may not have enough incentive to try.

The factors are: 1500 reward and 750 penalty (in general). 

\section*{\color{red} 23/5 18:25 (5\textunderscore 23\textunderscore 1825)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 256 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 1 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false\\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 203000

Time = 17000s

\vspace{2mm}

Reward=2000, fail=-500, miss=-1500. It has been corrected that it penalizes the final part of an impulse (if it succeeds, it does not continue to penalize).

It still doesn't have enough incentive to make impulses. Next time try with PPO and new parameters, and maybe also improve the reward system.

\section*{\color{red} 24/5 16:25 (5\textunderscore 24\textunderscore 1625)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 260000

Time = 8300s

\vspace{2mm}

PPO training. Reward=500, fail=-150, miss=-1500. Higher learning rate.

\section*{\color{red} 24/5 18:50 (5\textunderscore 24\textunderscore 1850)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			pretraining strength & 0.5 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.01 & gail gamma & 0.99\\
		\hline
			gail encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/QueueDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 323000

Time = 10750s

\vspace{2mm}

PPO training. Reward=750, fail=-250, miss=-1500. It does not perform impulses.

\section*{\color{red} 24/5 21:54 (5\textunderscore 24\textunderscore 2154)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 213000

Time = 5800s

\vspace{2mm}

PPO training. Reward=250, fail=-250, miss=-5000. Penalty for not performing impulses taken to the extreme. The program failed but the behavior didn't improve either.

\section*{\color{red} 25/5 16:07 (5\textunderscore 25\textunderscore 1607)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 5.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/BCDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 13000

Time = 4500s

\vspace{2mm}

Online training. A deterministic bot is used without any noise (constant movement at 0.3 and impulses at 0.2 seconds). It gets worse with time, although it doesn't start badly (if it catches just one impulse).

\section*{\color{red} 25/5 17:30 (5\textunderscore 25\textunderscore 1730)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & \textbf{learning rate} & \textbf{1.0e-5} \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/UniformDemo.demo}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 27000

Time = 8000s

\vspace{2mm}

Changed the demo and the learning rate. 

The reward decreases constantly, and the behavior gets worse, although in some specific moments it gave good results.

\section*{\color{red} 25/5 19:58 (5\textunderscore 25\textunderscore 1958)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 15000

Time = 4500s

\vspace{2mm}

Now checks if there's a non-null target. There's been a bot failure, it's repeated.

\section*{\color{red} 25/5 21:57 (5\textunderscore 25\textunderscore 2157)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 256 \\ 
		\hline
			num layers & 2 & sequence length & 32 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 46000

Time = 13300s

\vspace{2mm}

Several corrections to the bot's behavior (at some point it stopped reacting to targets, because the closest variable didn't restart).

It gets worse over time, making very strange effects after pulling several impulses. It is saved to test it in execution.

It does a good continuous movement, but when there's a target between the previous frames it oscillates in strange ways, although it keeps moving in the direction it should (as a whole). PPO, or more complex layers with imitation learning, should be tried.

\section*{\color{red} 26/5 10:59 (5\textunderscore 26\textunderscore 1059)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 122000

Time = 4600s

\vspace{2mm}

Training with a single PPO thread (not multi) It should get better results since the movement is deterministic. The real bot does strange behaviors so it is stopped.

\section*{\color{red} 26/5 12:25 (5\textunderscore 26\textunderscore 1225)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 200000

Time = 7750s

\vspace{2mm}

By setting the distance of closest to 2.0f, it reacted earlier (corrected). It does not seem to converge with PPO.

\section*{\color{red} 26/5 15:07 (5\textunderscore 26\textunderscore 1507)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			normalize & true & num epoch & 6\\
		\hline
			num layers & 1 & summary freq & 1000\\
		\hline
			time horizon & 64 & &\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 460000

Time = 17200s

\vspace{2mm}

Use of simpler linear rewards, assuming the movement is deterministic and has no noise.

The reward grows slowly, and catches the impulses more or less well but may lack the complexity of the network structure to learn well.

\section*{\color{green} 26/5 19:58 (5\textunderscore 26\textunderscore 1958)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & \textbf{memory size} & \textbf{32} \\ 
		\hline
			\textbf{num layers} & \textbf{3} & \textbf{sequence length} & \textbf{128} \\ 
		\hline
			summary freq & 1000 & \textbf{use recurrent} & \textbf{true} \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformDemo.demo

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 12000

Time = 13000s

\vspace{2mm}

More complex network with imitation learning.
The training time is very slow, but the results are good. However, the model loads the CPU a lot when inferring, decreasing the fps by half (25-30). When using recurrent memory, it may not be necessary to save previous movements and images.

As a bad part, from time to time random impulses appear without any target in sight, they may disappear with more training or they may occur due to lack of complexity (react time is 0.25). 

In other experiments, different architectures could be tested using recurrent (1, 2 or 4 layers, with more or less hidden units), and some without more inputs than the current image (and maybe the previous motion, depending on how the recurrent memory works).

\section*{\color{red} 27/5 09:53 (5\textunderscore 27\textunderscore 0953)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			num layers & 3 & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & true \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/UniformNoRTDemo.demo}

\vspace{2mm}

\textbf{Render Target Sensor: 1 * (20x20)}

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 18000

Time = 16500s

\vspace{2mm}

From tests carried out, it seems that BC cannot be used with a vector observation of size 0 (I have left it at 25 as it was). It's also important to point out that it is necessary to turn off the demonstration recorder to train, and that it is possible to train online with PPO and SAC (I suppose that in a single thread).

Compare the training speed (and results) with the previous model, and the FPS when running it.

The result looks good, at the expense of testing it. It has the typical random impulses, we will have to see if it is more efficient too.

In the editor it goes crazy after a few seconds, maybe because it receives the empty previous movement vector. Try to do it with less movements.

Reaction 0.25.

\section*{\color{red} 27/5 16:56 (5\textunderscore 27\textunderscore 1656)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			\textbf{memory size} & \textbf{32} & normalize & true\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			\textbf{sequence length} & \textbf{128} & summary freq & 1000 \\
		\hline
			time horizon & 64 & \textbf{use recurrent} & \textbf{true}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

\textbf{Observation space size = 1}

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 70000

Time = 1550s

\vspace{2mm}

An enumeration has been created to support more types of observations, in this case it only receives the last movement. Attempting to train with imitation learning does not work using only one observation (with 25).

In this training we use PPO with recurrent memory, and the last movement as visual input. It is done online. However, it has less layers than the previous ones with imitation learning.

I cancel it to try with SAC, it doesn't seem to improve the previous PPO results. 

\section*{\color{red} 27/5 17:27 (5\textunderscore 27\textunderscore 1727)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & \textbf{hidden units} & \textbf{128} \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			\textbf{memory size} & \textbf{32} & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{2} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & \textbf{use recurrent} & \textbf{true} \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/UniformNoRT1Demo.demo}

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 235000

Time = 24000s

\vspace{2mm}

Online training with SAC and 2 layers in the network. 

It looked like it was going to converge to the average, but in the end it has started to make very irregular impulses. It may work with more network complexity.

\section*{\color{red} 28/5 12:40 (5\textunderscore 28\textunderscore 1240)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 2 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = -

Time = -

\vspace{2mm}

--Tests

Training with higher requirements in rewards and more layers.

Does not work with 1 observation and 2 or 3 layers. The error seems to have to do with sequence length, I don't know if it should not be greater than the hidden layer units or if the relationship between memory size and sequence length should be kept to a minimum. I'm going to do some tests (with sequence length=128, like the hidden units, it works with an observation). The curious thing is that the failures were in step 5000.

\vspace{2mm}
sl=256, hu=128, ms=32: FAILS

sl=256, hu=256, ms=32: FAILS, must not be for equality with hidden units

sl=256, hu=128, ms=64: FAILS

sl=128, hu=128, ms=16: DOES NOT FAIL
\vspace{2mm}

For some reason it always fails when sequence length is greater than or equal to 256, the buffer size is also not the key. Therefore, the most that can be used is 128.

Could the encoding size of curiosity and gail be the cause, since both were 128? Test it later.

\section*{\color{red} 28/5 13:16 (5\textunderscore 28\textunderscore 1316)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{3} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 272000

Time = 28000s

\vspace{2mm}

It didn't look bad after about 150000+ steps, it went from converging to average to start making impulses (with some noise), but then it started to go crazy. However, the reward has increased, so I think there may be a problem that makes the rewards not very well balanced. Either that, or it may require some additional layer.

I've checked that sequence length also fails with equal encoding sizes, so it can't be 256 in general.

\section*{\color{red} 28/5 21:08 (5\textunderscore 28\textunderscore 2108)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{4} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 100000

Time = 10300s

\vspace{2mm}

One more layer in the network. I stop the training but it seems to be following the same path as the previous one. I could try again by combining the use of recurrent and the previous images.

Despite having one more layer, it seems to have no added cost. It would be nice to continue with this same training (--load)

\section*{\color{red} 29/5 09:26 (5\textunderscore 29\textunderscore 0926)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			num layers & 3 & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & true \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/UniformNoRT1Demo.demo}

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 18000

Time = 17200s

\vspace{2mm}

BC training using an input. The rest of the parameters are the same as in the last training that worked.

Testing if it works well in the editor, the result seemed correct (at least with the ones on the left).

In editor it doesn't work well after encountering a left-hand pulse: when it stands still it enters untrained cases.

\section*{\color{red} 29/5 15:04 (5\textunderscore 29\textunderscore 1504)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 128 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			\textbf{num layers} & \textbf{2} & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & true \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 25

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 23000

Time = 21500s

\vspace{2mm}

BC training and one less layer. It's very similar to the previous one. 

In the game it stops working in about 2 seconds, when it starts spinning uncontrollably.

\section*{\color{red} 29/5 21:26 (5\textunderscore 29\textunderscore 2126)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 4.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{2} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 90000

Time = 9000s

\vspace{2mm}

Training with adjusted rewards (squared). Although the demo has not been changed, so it will have old rewards there. In this first test we only use 2 layers.

It's close to average, but it doesn't make any impulses.

\section*{\color{red} 31/5 19:29 (5\textunderscore 31\textunderscore 1929)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & \textbf{learning rate} & \textbf{7.5e-4}\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{3} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.5 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 153000

Time = 15700s

\vspace{2mm}

Higher learning rate. In following trainings it would be good to base the rewards on coherence (adapted to a single input). It has grown faster but falls back after 90k steps. It doesn't get to make impulses as far as I've seen, it would be convenient to search among previous models to readapt them.

\section*{\color{red} 01/6 10:50 (6\textunderscore 01\textunderscore 1050)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & \textbf{learning rate} & \textbf{5.0e-4}\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			\textbf{extrinsic strength} & \textbf{1.0} & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.03 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 190000

Time = 19700s

\vspace{2mm}

Training with higher gail and curiosity weight (in ratio) and reward factor of 300 (against 10). Seems to be stagnating on the same behaviors.

\section*{\color{red} 01/6 16:23 (6\textunderscore 01\textunderscore 1623)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			\textbf{curiosity strength} & \textbf{0.1} & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Demo path: demos/UniformNoRT1Demo.demo

\vspace{2mm}

Render Target Sensor: 1 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 305000

Time = 32000s

\vspace{2mm}

It goes up until it adapts to the average (without impulses) but then it goes down again, it is not useful to use recurrent memory in these cases without other information (it might work but with a much more perfected system of rewards). Next time try the combination of render targets and recurrent but with SAC (and a single motion input). 

\section*{\color{red} 02/6 10:23 (6\textunderscore 02\textunderscore 1023)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\textbf{Demo path: demos/Frames20Demo.demo}

\vspace{2mm}

\textbf{Render Target Sensor: 20 * (20x20)}

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 90000

Time = 18000s

\vspace{2mm}

Training with 20 render targets, the demo has also been updated. The objective is to see if it improves the performance with respect to the one trained with SAC. It is still only adjusted to the average, it doesn't seem to have any incentive to make impulses. In the next training I will use as input the movements of the bot to see how long it takes to learn (if it does), if it does not learn, change reward system (even to TR) and if it learns, check the reward in which it behaves decently to incorporate learning curriculum.

\section*{\color{red} 02/6 15:45 (6\textunderscore 02\textunderscore 1545)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

Demo path: demos/Frames20Demo.demo

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 86000

Time = 17600s

\vspace{2mm}

Receives actual values from the bot. Doesn't react anyway.

Add noise in next tests.

\section*{\color{red} 02/6 20:54 (6\textunderscore 02\textunderscore 2054)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\textbf{Demo path: demos/R0N02.demo}

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 55000

Time = 9500s

\vspace{2mm}

Parameters in the bot (reaction=0, noise=0.2) and rewards with tolerable range (TR=0.05, MR=1), as a reference point to see if it works. The demo has an identifying name.
This model doesn't work because there was an error in the tolerable range rewards that made it always give 0 as a punishment.

\section*{\color{red} 02/6 23:41 (6\textunderscore 02\textunderscore 2341)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

Demo path: demos/R0N02.demo

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 100000

Time = 7300s + 11000s

\vspace{2mm}

Same parameters but corrected. The demo is redone because the previous one has incorrect rewards. Also the noise has been reduced to 0.05 to enter the tolerable range.

Training interrupted, continue.

Next models could be trained with PPO, as it should be faster in reaching a certain level. It doesn't seem to go too high, and it may be caused by the resolution of the render targets (by discard). In the next test PPO.

\section*{\color{red} 03/6 12:41 (6\textunderscore 03\textunderscore 1241)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.0e-3\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & true\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 154000

Time = 7000s

\vspace{2mm}

PPO training with the parameters that were. No demo included.

Converge to the average, no impulses. Next I try with 64x64 render targets.

\section*{\color{red} 03/6 14:54 (6\textunderscore 03\textunderscore 1454)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{2.5e-4}\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & \textbf{normalize} & \textbf{false}\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & true\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Render Target Sensor: 64*64}

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 14000s

\vspace{2mm}

Change in render target, normalize and learning rate. If it works well it serves as a reference, if not there is a problem with another parameter or the reward system.

It didn't work. Next time I'll try with rewards with coherence (that I should create).

\section*{\color{red} 05/6 12:05 (6\textunderscore 05\textunderscore 1205)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

Demo path: demos/Frames20Demo.demo

\vspace{2mm}

\textbf{Render Target Sensor: 20 * (20x20)}

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 82000

Time = 15900s

\vspace{2mm}

First training after migrating repository. It has given some error at the beginning for not finding references (to something of python or ml agents).

Now I use rewards inside range (they are similar to TR, but they give maximum reward as long as it is within range).

It gets close to the average as expected.

\section*{\color{red} 05/6 16:59 (6\textunderscore 05\textunderscore 1659)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & true \\
		\hline
			vis encode type & simple & & \\
		\hline
			pretraining strength & 0.4 & pretraining steps & 20000\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
			gail strength & 0.03 & gail gamma & 0.99 \\
		\hline
			gail encoding size & 128 & gail use actions & true \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 5, 10, 12, 15, 17, 20, 22, 25, 27, 30, 32, 35, 40, 45, 50, 55, 60, 70, 80

Demo path: demos/Frames20Demo.demo

\vspace{2mm}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 117000

Time = 25200s

\vspace{2mm}

Change in Rewards (StdCoherenceIndividual): Failing coherent moves penalizes, and succeeding inconsistent moves gives reward (penalizes only if done in the opposite direction). The average is a bit unstable but should be fine. In future models it would be good to use dynamic averaging and deviation.

It doesn't converge.

\section*{\color{red} 06/6 14:33 (6\textunderscore 06\textunderscore 1433)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & true\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 20 * (20x20)

Grayscale: true

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 137000

Time = 6200s

\vspace{2mm}

Dynamic average and deviation implemented. The previous reward is used but now takes into account half of the deviation to start penalizing (that's how far it usually expands).

It tends to exploit the rewards by staying in the margin where the impulses usually appear. Needs more penalty.

\section*{\color{red} 06/6 16:30 (6\textunderscore 06\textunderscore 1630)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & true\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Render Target Sensor: 1 * (20x20)}

\textbf{Grayscale: false}

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 487000

Time = 10000s

\vspace{2mm}

Slight changes in the linearity of rewards and factors Now I go back to NOT using grayscale in the rendering. In next tests I may put the previous 25 moves back.

Failed training by leaving the reaction time at 0.2 by mistake.

\section*{\color{red} 06/6 19:33 (6\textunderscore 06\textunderscore 1933)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & true\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (20x20)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 411000

Time = 9000s

\vspace{2mm}

Training with react time = 0, in theory it should converge, unless the rewards are not yet balanced.

It doesn't converge at all.

\section*{\color{green} 06/6 23:19 (6\textunderscore 06\textunderscore 2319)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & \textbf{use recurrent} & \textbf{false}\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Render Target Sensor: 1 * (64x64)}

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 432000

Time = 9400s

\vspace{2mm}

This time I take away recurrent and double the reward factor (50 against 100).

It has failed a training (it has stopped in 8000 steps, but without giving error).

It has managed to follow impulses with great precision but it lacks a jump in the penalties so that it tends to the average in the coherent movements.

In addition, the buffer size may have to be decreased to avoid errors like the previous one (check how it was in previous successful models).
Keep in mind that using the model as it is will probably cause it to diverge (since it has only trained with real values). In the next training the result of using the bot values instead of the real ones could be compared.

\section*{\color{green} 07/6 10:00 (6\textunderscore 07\textunderscore 1000)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 8192 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 235000

Time = 5000s

\vspace{2mm}

In this training it receives the observations of the bot itself. This allows you to compare the learning curve of both.

Remember to change the linearity of the rewards after this training, and the buffer size.

It follows exactly the same path as the previous model.

\section*{\color{green} 07/6 11:39 (6\textunderscore 07\textunderscore 1139)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{4096} & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 588000

Time = 12700s

\vspace{2mm}

More pressure has been added to the rewards, and 2 jumps in value. The parameters are still 50 reward and 100 punish, and the agent's observations. The buffer has also been changed. The result seems similar to the previous 2.

\section*{\color{green} 07/6 15:15 (6\textunderscore 07\textunderscore 1515)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 418000

Time = 9000s

\vspace{2mm}

Even more penalty pressure has been added (it now always penalizes inconsistent movement, unless it is exactly the same, and linearly increasing with distance). If it doesn't work, the reward/penalty factors will have to be adjusted. The other parameters are the same.

It works the same as the previous ones, that's why it is not saved.

\section*{\color{green} 07/6 17:47 (6\textunderscore 07\textunderscore 1747)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 1 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = -

Time = -

\vspace{2mm}

Penalty factor at 150 and reward factor at 50.

Up to 230000 steps (-20 reward) follows all similar (makes perfect impulses but has a lot of noise in normal movement), so I'm going to increase the penalty to 250 at this point. The reward goes down to -42, if it evolves well from now on it may work as a learning curriculum, if it follows the same trend it will be better to change the form of the penalties and/or use SAC (Also keep in mind that the learning rate has already gone down).

It does not change.

\section*{\color{red} 07/6 19:47 (6\textunderscore 07\textunderscore 1947)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & \textbf{use recurrent} & \textbf{false} \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 316000

Time = 11000s

\vspace{2mm}

Training with SAC (without GAIL) and the same initial parameters as in the previous one (also without recurrent and with observations of the agent).

It has reached a good reward but after 107k steps it has fallen (it has remained flat). I think the problem is that the reward is also unbalanced, it should have minimal rewards for getting very close in coherent movements, so that a positive result can be achieved.

\section*{\color{red} 08/6 10:54 (6\textunderscore 08\textunderscore 1054)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 161000

Time = 5200s

\vspace{2mm}

Training until I fail, to see the results of before.

It's much worse with SAC, I think that because the coherent movement has no positive reward, it doesn't distinguish when an impulse fails from when a coherent movement succeeds and ends up being fun. In the next one I'll add low rewards to see if it doesn't diverge.

\section*{\color{red} 08/6 12:32 (6\textunderscore 08\textunderscore 1232)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 128 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 92000

Time = 3200s

\vspace{2mm}

Minimal rewards added. Still not converging, the next time I test PPO but with 3 layers.

\section*{\color{red} 08/6 13:28 (6\textunderscore 08\textunderscore 1328)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 2.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & \textbf{num layers} & \textbf{3} \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.01 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 13000s

\vspace{2mm}

PPO with the new minimum rewards and 3 layers. It gets the same results as before.

\section*{\color{red} 08/6 17:25 (6\textunderscore 08\textunderscore 1725)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{1.5e-4}\\
		\hline
			\textbf{learning rate schedule} & \textbf{constant} & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & \textbf{num layers} & \textbf{3} \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & \textbf{extrinsic gamma} & \textbf{0.8}\\
		\hline
			\textbf{curiosity strength} & \textbf{0.02} & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 76000

Time = 1650s

\vspace{2mm}

Very unbalanced exponential rewards.

\section*{\color{green} 08/6 17:59 (6\textunderscore 08\textunderscore 1759)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 13000s

\vspace{2mm}

Rewards with decay from $x^5$ to 1 distance, then linear. It seems that the weight of the penalties stays very high now.
The result is still similar, but the rewards are much lower.

\section*{\color{green} 08/6 21:52 (6\textunderscore 08\textunderscore 2152)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 13000s

\vspace{2mm}

Reward=100, penalty=150. In addition, it receives observations from the bot only. Change decay if it still doesn't work.

Although it still has a lot of noise, the impulses are traced millimetrically (both when the network moves and when it imitates the bot), although it may be because it receives the bot's values as input (they are uniform) in spite of not really being working. If it works well we could start testing with some fixed delay frames (receiving all the following ones as observation) and then increase them.

\section*{\color{green} 09/6 11:19 (6\textunderscore 09\textunderscore 1119)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 585000

Time = 12750s

\vspace{2mm}

Noise from 0.05 to 0.15, and now receives observations from the agent instead of the bot.

I could try to put the average as a target, instead of the real value (in coherent movements).

The bot adapts to a level similar to the previous bots, the standard deviation is similar in spite of having more noise. You could smooth out the deviation somewhat to make it better suited to the movement (even though it's not exactly a standard deviation).

\section*{\color{red} 09/6 15:03 (6\textunderscore 09\textunderscore 1503)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 356000

Time = 7700s

\vspace{2mm}

Same parameters as before, but using a reward system that appreciates being closer to the average more than the real movement (it receives rewFactor/10, in a fixed way). It might cause the bot not to be motivated to follow impulses, but I hope it will be more adapted to the average, even if it's too much.

It seems that there is no change, I will increase the amount of reward for successful coherent movement.

\section*{\color{red} 09/6 17:15 (6\textunderscore 09\textunderscore 1715)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 13000s

\vspace{2mm}

It now receives the same factor for the coherent as the incoherent (the coherent may be even greater), and the reward and penalty factors are the same (100).

In spite of the changes, it continues to do well on impulses (although it gets worse on the right) and the coherent movement has not changed much. 

\section*{\color{red} 09/6 21:42 (6\textunderscore 09\textunderscore 2142)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 1.5e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 297000

Time = 6400s

\vspace{2mm}

Doubled the reward factor to 200 but the proportion that receives the consistent reward movement has been halved (0.5).

With these values it does not catch the right impulses well, the results are similar to the previous checkpoint.

Although the impulses are not perfectly modeled, in future tests we could start using bots with delays.

\section*{\color{red} 10/6 11:48 (6\textunderscore 10\textunderscore 1148)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & \textbf{learning rate} & \textbf{7.5e-4}\\
		\hline
			learning rate schedule & constant & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 340000

Time = 7400s

\vspace{2mm}

I've added a consistency factor to the rewards within the average. The factors are 150 of penalty and 50 of reward. It also has a smoothing of deviations, and a higher learning rate.

The result was quite good and rising fast, but diverges at 320k steps (the learning rate schedule was constant, so it was to be expected). Although the right hand impulses were not always right.

\section*{\color{green} 10/6 14:04 (6\textunderscore 10\textunderscore 1404)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			\textbf{learning rate schedule} & \textbf{linear} & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Render Target Sensor: 1 * (64x64)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 13000s

\vspace{2mm}

Noise at 0.3, same parameters.

Pretty good result, it fits well to the 2 movements, although it makes some impulse from the right loosely.

\section*{\color{green} 10/6 18:54 (6\textunderscore 10\textunderscore 1854)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

\textbf{Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9}

\vspace{2mm}

\textbf{Render Target Sensor: 10 * (32x32)}

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 20000s

\vspace{2mm}

0.1 of react time and 0.2 of noise. All other parameters same as above. Now it receives as input the frames 1-2-3-4-5-6-7-8-9 (apart from the current one). The reaction should occur in frame 5-6. It may take longer to train, as it now receives 10 visual inputs instead of 1, and that can generate a lot of noise (it may also be better to give it the last 10 actions as input).

Using 64x64 inputs gives "Cuda out of memory" error, so I have decreased the size to 32 to redo the training.

The bot adapts well to most movements, both coherent and incoherent. However, some rightward impulses are not caught, and some consecutive movements are not as good either.

Training time has increased by 50 percent. Taking into account the amount of processed pixels: 64*64 = 4096, 32*32 * 10 = 10240. That's slightly more than double.

\section*{\color{red} 11/6 12:47 (6\textunderscore 11\textunderscore 1247)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 1024 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 4096 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 6 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 173000

Time = 8000s

\vspace{2mm}

0.12 of react time and 0.05 of noise. The render queue has also been optimized. There have been some out of memory failures, so I should see how to increase it.

Failure after several steps (sync error), probably because of the same thing of the memory. 

\section*{\color{red} 11/6 15:40 (6\textunderscore 11\textunderscore 1540)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			\textbf{batch size} & \textbf{512} & beta & 5.0e-3 \\ 
		\hline
			\textbf{buffer size} & \textbf{2048} & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			\textbf{num epoch} & \textbf{4} & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 291000

Time = 10000s

\vspace{2mm}

The batch size and buffer size have been decreased to check if they consume less memory. Training may be less stable (although the num epoch has been lowered, which should make it more stable at the cost of being slower).

For some reason Unity fails and closes. It happens after optimizing the render targets.

\section*{\color{green} 12/6 10:40 (6\textunderscore 12\textunderscore 1040)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 512 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 2048 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 4 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 20200s

\vspace{2mm}

The error seems to have its origin in the fact that the render targets created are not deleted/destroyed (the garbage collector doesn't collect them). Now I've put the destroy sequence, but if it still fails we'll have to call GC.Collect or Resources.UnloadUnusedAssets too.
The training is the same as the previous one. It reaches the same score but more slowly.

In this model, the standard deviation is a little bit higher, and the bot tends to adapt to it, so it has more noise than the original one.

\section*{\color{green} 12/6 18:12 (6\textunderscore 12\textunderscore 1812)}

Trainer: PPO

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 512 & beta & 5.0e-3 \\ 
		\hline
			buffer size & 2048 & epsilon & 0.2 \\
		\hline
		  hidden units & 256 & learning rate & 7.5e-4\\
		\hline
			learning rate schedule & linear & max steps & 6.0e5\\
		\hline
			memory size & 32 & normalize & false\\
		\hline
			num epoch & 4 & num layers & 3 \\
		\hline
			sequence length & 128 & summary freq & 1000 \\
		\hline
			time horizon & 64 & use recurrent & false\\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.8\\
		\hline
			curiosity strength & 0.02 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 600000

Time = 19700s

\vspace{2mm}

React time of 0.08 and noise of 0.15. In this bot the spawner has been changed to balance the number of times each case is encountered, for the moment the same amount each of the 4 (chosen at random). Theoretically it should train faster and get a better result. 

The spawns on the right keep failing.

\section*{\color{red} 13/6 10:47 (6\textunderscore 13\textunderscore 1047)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & \textbf{hidden units} & \textbf{256} \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 74000

Time = 5000s

\vspace{2mm}

Training with SAC. The proportion of cases has been changed (more or less 50\% left, and 24\% consecutive).

It doesn't work, it stays in the average. It may occur because of receiving rewards in the center.

\section*{\color{red} 13/6 12:24 (6\textunderscore 13\textunderscore 1224)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			num layers & 3 & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 52000

Time = 3500s

\vspace{2mm}

I have removed the consistency factor on rewards within the average (this will apply to subsequent PPO trainings). In this one it does not receive any reward for getting the coherent movement right (prop. coherent = 0).

Now it doesn't seem to converge. It is kept as it is, the agent serves as an example when the problem is too complex for the number of layers used.

\section*{\color{red} 13/6 13:27 (6\textunderscore 13\textunderscore 1327)}

Trainer: SAC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batch size & 128 & buffer size & 200000 \\ 
		\hline
			buffer init steps & 5000 & hidden units & 256 \\
		\hline
			init entcoef & 1.0 & learning rate & 5.0e-4\\
		\hline
			learning rate schedule & constant & max steps & 6.0e6\\
		\hline
			memory size & 32 & normalize & true\\
		\hline
			num update & 1 & train interval & 5\\
		\hline
			\textbf{num layers} & \textbf{4} & time horizon & 64\\
		\hline
			sequence length & 128 & summary freq & 1000\\
		\hline
			tau & 0.005 & use recurrent & false \\
		\hline
			vis encode type & simple & & \\
		\hline
			extrinsic strength & 1.0 & extrinsic gamma & 0.99\\
		\hline
			curiosity strength & 0.1 & curiosity gamma & 0.99\\
		\hline
			curiosity encoding size & 128 & & \\
		\hline
	\end{tabular}
\end{center}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 78000

Time = 5500s

\vspace{2mm}

Same parameters but one more layer in the network. If it doesn't improve the result, the problem may be in the visual encoder.

It follows the same trends but takes longer to get there. I don't know if it could end up converging but it doesn't seem to.

\section*{\color{red} 13/6 15:06 (6\textunderscore 13\textunderscore 1506)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			\textbf{hidden units} & \textbf{256} & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			\textbf{num layers} & \textbf{3} & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & \textbf{use recurrent} & \textbf{false} \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/R008N015.demo}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 11000

Time = 2500s

\vspace{2mm}

Because of an initial error, I believe that recurrent memory does not go well when it receives an observation. At the end it diverges, I'm going to try 4 layers.

\section*{\color{red} 13/6 15:55 (6\textunderscore 13\textunderscore 1555)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 256 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			\textbf{num layers} & \textbf{4} & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/R008N015.demo

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 17000

Time = 4200s

\vspace{2mm}

It has a very erratic behavior, it doesn't get any impulse right. Special cases make the problem more complex to solve (and I think we will have to change the visual encoder).

\section*{\color{green} 13/6 17:20 (6\textunderscore 13\textunderscore 1720)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 256 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			num layers & 4 & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/R01N015T005.demo}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 7000

Time = 1500s

\vspace{2mm}

A time deviation variable has been added (to 0.05). The noise is now 0.15 and the reaction time is 0.1. Only the common case is taken into account in this training, for simplicity.

It seems to work decently, it removes the noise and hits the impulses.
Note** it had been trained with the bot's observations instead of its own, strange cases occur when it reacts earlier (but receives observations otherwise).

\section*{\color{green} 13/6 18:00 (6\textunderscore 13\textunderscore 1800)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 256 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			num layers & 4 & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

\textbf{Demo path: demos/R001N015T014.demo}

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 7000

Time = 2000s

\vspace{2mm}

Extreme case: it can react in any of the frames it receives (from 0.01 to 0.15). Reaction time at 0.01 (temporary noise 0.14) and noise at 0.15.

When left longer it loses precision.

\section*{\color{green} 13/6 18:43 (6\textunderscore 13\textunderscore 1843)}

Trainer: BC

\begin{center}
	\begin{tabular}{ | m{4cm} | m{2.5cm}||m{4cm} | m{2.5cm} | } 
		\hline
			batches per epoch & 10 & batch size & 64 \\ 
		\hline
			hidden units & 256 & learning rate & 1.0e-5 \\ 
		\hline
			max steps & 5.0e4 & memory size & 32 \\ 
		\hline
			num layers & 4 & sequence length & 128 \\ 
		\hline
			summary freq & 1000 & use recurrent & false \\ 
		\hline
	\end{tabular}
\end{center}

Demo path: demos/R001N015T014.demo

Frames: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9

\vspace{2mm}

Render Target Sensor: 10 * (32x32)

Grayscale: false

Observation space size = 1

Action space size = 1

\vspace{2mm}

Episode steps = 1000

Total steps = 2000

Time = 500s

\vspace{2mm}

Training with the same parameters as above but shorter. It has a much better result than the previous ones, from which conclusions could be drawn about how well it models behaviour. The next steps would be to apply it to bots with a higher range (and where the agent does not receive all the frames), probably with a higher number of observations of previous movements, to test other encoders (along with more cases), to check the performance of BC networks against PPOs (SAC did not work out well), etc.